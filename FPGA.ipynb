{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAyzqEmp-5y3",
        "outputId": "71d03f28-9bec-4dda-fee6-53f10ec6ff6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "Warning: Failed to create the file /root/content/cicddos2019.zip: No such file \n",
            "Warning: or directory\n",
            "  0 28.7M    0  1177    0     0   2114      0  3:57:37 --:--:--  3:57:37  2114\n",
            "curl: (23) Failure writing output to destination\n"
          ]
        }
      ],
      "source": [
        "#!/bin/bash\n",
        "!curl -L -o ~/content/cicddos2019.zip\\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/dhoogla/cicddos2019"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qeh42l_-8X8",
        "outputId": "b6f5c5d4-da6a-4d86-c39d-19600d1795a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/dhoogla/cicddos2019\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading cicddos2019.zip to /content\n",
            " 63% 18.0M/28.7M [00:00<00:00, 52.3MB/s]\n",
            "100% 28.7M/28.7M [00:00<00:00, 75.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download dhoogla/cicddos2019"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9BxVRdG_PyA",
        "outputId": "c0ea3f0c-bbdb-4e39-9adf-bf7e52b48bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping completed!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "zip_file_path = \"/content/cicddos2019.zip\"\n",
        "extract_to_path = \"/content/data/\"\n",
        "\n",
        "# Ensure the target directory exists\n",
        "os.makedirs(extract_to_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)\n",
        "\n",
        "print(\"Unzipping completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE8U9TaE_acm",
        "outputId": "aea8311d-f6c9-4454-9874-548245f83e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files concatenated and saved as data.parquet.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to your Parquet files directory\n",
        "path = \"/content/data\"  # Replace with the actual directory path\n",
        "\n",
        "# List all parquet files\n",
        "parquet_files = glob.glob(os.path.join(path, \"*.parquet\"))\n",
        "\n",
        "# Concatenate all parquet files\n",
        "dataframes = [pd.read_parquet(file) for file in parquet_files]\n",
        "data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Save the concatenated data to a new file\n",
        "output_file = \"data.parquet\"\n",
        "data.to_parquet(output_file, index=False)\n",
        "\n",
        "print(f\"All files concatenated and saved as {output_file}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrlSnDpNvQal",
        "outputId": "c3f6bf84-e6f1-45ce-bc05-ac4d40924886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting zynet\n",
            "  Downloading zynet-0.4-py3-none-any.whl.metadata (211 bytes)\n",
            "Downloading zynet-0.4-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: zynet\n",
            "Successfully installed zynet-0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install zynet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "EYgiXkN5vNK8",
        "outputId": "278e013b-64b4-4585-dbe8-362ce960e816"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'zynet'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f69f533300a5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mzynet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_for_verilog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Selected features based on the data you shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'zynet'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from zynet import Sequential, Dense, Dropout, softmax, compile_for_verilog\n",
        "\n",
        "# Selected features based on the data you shared\n",
        "selected_features = [\n",
        "    'Protocol',\n",
        "    'Flow Duration',\n",
        "    'Total Fwd Packets',\n",
        "    'Total Backward Packets',\n",
        "    'Fwd Packets Length Total',\n",
        "    'Bwd Packets Length Total',\n",
        "    'Fwd Packet Length Max',\n",
        "    'Fwd Packet Length Min',\n",
        "    'Fwd Packet Length Mean',\n",
        "    'Fwd Packet Length Std'\n",
        "]\n",
        "\n",
        "target_column = 'Label'\n",
        "\n",
        "# Load data\n",
        "data = pd.read_parquet(\"/content/data.parquet\")\n",
        "print(\"Data shape after concatenation:\", data.shape)\n",
        "\n",
        "# Encode the target variable\n",
        "le_target = LabelEncoder()\n",
        "data[target_column] = le_target.fit_transform(data[target_column])\n",
        "\n",
        "# Split features and target\n",
        "X = data[selected_features]\n",
        "y = data[target_column]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build and train the neural network using ZyneT\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_dim=X_train.shape[1]),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(len(le_target.classes_), activation=softmax)  # Multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Convert the trained model to Verilog code for FPGA/ASIC implementation\n",
        "verilog_code = compile_for_verilog(model)\n",
        "\n",
        "# Save or print the generated Verilog code\n",
        "with open(\"neural_network.v\", \"w\") as verilog_file:\n",
        "    verilog_file.write(verilog_code)\n",
        "\n",
        "print(\"Verilog code generated and saved as 'neural_network.v'\")\n",
        "\n",
        "# Extract weights and biases from the trained model\n",
        "with open(\"WeightsAndBiases.txt\", \"w\") as weights_file:\n",
        "    weights_file.write(\"Weights and Biases of the Trained Neural Network:\\n\\n\")\n",
        "\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if hasattr(layer, \"weights\"):  # Check if the layer has weights\n",
        "            weights = layer.get_weights()\n",
        "\n",
        "            if weights:\n",
        "                weights_file.write(f\"Layer {i + 1}: {layer.name}\\n\")\n",
        "\n",
        "                if len(weights) > 0:\n",
        "                    weights_file.write(\"Weights:\\n\")\n",
        "                    weights_file.write(str(weights[0]))\n",
        "                    weights_file.write(\"\\n\\n\")\n",
        "\n",
        "                if len(weights) > 1:\n",
        "                    weights_file.write(\"Biases:\\n\")\n",
        "                    weights_file.write(str(weights[1]))\n",
        "                    weights_file.write(\"\\n\\n\")\n",
        "\n",
        "print(\"Weights and biases saved in 'WeightsAndBiases.txt'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "bWE0bLwjxWRw",
        "outputId": "6ac75eef-ac8d-4d89-c256-ceb33ea20818"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'WeigntsAndBiases.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-21c0a213f11e>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgenMnistZynet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataWidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigmoidSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweightIntSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputIntSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-21c0a213f11e>\u001b[0m in \u001b[0;36mgenMnistZynet\u001b[0;34m(dataWidth, sigmoidSize, weightIntSize, inputIntSize)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzynet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dense\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzynet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dense\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"hardmax\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mweightArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenWeightArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'WeigntsAndBiases.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mbiasArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenBiasArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'WeigntsAndBiases.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Yes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweightArray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbiasArray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataWidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataWidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweightIntSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweightIntSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputIntSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputIntSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigmoidSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigmoidSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zynet/utils.py\u001b[0m in \u001b[0;36mgenWeightArray\u001b[0;34m(fileName)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenWeightArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mweightFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmyData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweightFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmyDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'WeigntsAndBiases.txt'"
          ]
        }
      ],
      "source": [
        "from zynet import zynet\n",
        "from zynet import utils\n",
        "import numpy as np\n",
        "\n",
        "def genMnistZynet(dataWidth,sigmoidSize,weightIntSize,inputIntSize):\n",
        "    model = zynet.model()\n",
        "    model.add(zynet.layer(\"flatten\",10))\n",
        "    model.add(zynet.layer(\"Dense\",30,\"sigmoid\"))\n",
        "    model.add(zynet.layer(\"Dense\",20,\"sigmoid\"))\n",
        "    model.add(zynet.layer(\"Dense\",10,\"sigmoid\"))\n",
        "    model.add(zynet.layer(\"Dense\",10,\"hardmax\"))\n",
        "    weightArray = utils.genWeightArray('WeigntsAndBiases.txt')\n",
        "    biasArray = utils.genBiasArray('WeigntsAndBiases.txt')\n",
        "    model.compile(pretrained='Yes',weights=weightArray,biases=biasArray,dataWidth=dataWidth,weightIntSize=weightIntSize,inputIntSize=inputIntSize,sigmoidSize=sigmoidSize)\n",
        "    zynet.makeXilinxProject('myProject1','xc7z020clg484-1')\n",
        "    zynet.makeIP('myProject1')\n",
        "    zynet.makeSystem('myProject1','myBlock2')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    genMnistZynet(dataWidth=8,sigmoidSize=10,weightIntSize=4,inputIntSize=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "fHF_YznLcLc8",
        "outputId": "5e6fa064-7994-4348-a757-30965342ff20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Protocol  Flow Duration  Total Fwd Packets  Total Backward Packets  \\\n",
              "0        17              1                  2                       0   \n",
              "1        17              1                  2                       0   \n",
              "2        17              1                  2                       0   \n",
              "3        17              1                  2                       0   \n",
              "4        17              2                  2                       0   \n",
              "\n",
              "   Fwd Packets Length Total  Bwd Packets Length Total  Fwd Packet Length Max  \\\n",
              "0                     422.0                       0.0                  211.0   \n",
              "1                     458.0                       0.0                  229.0   \n",
              "2                     458.0                       0.0                  229.0   \n",
              "3                     494.0                       0.0                  247.0   \n",
              "4                     458.0                       0.0                  229.0   \n",
              "\n",
              "   Fwd Packet Length Min  Fwd Packet Length Mean  Fwd Packet Length Std  ...  \\\n",
              "0                  211.0                   211.0                    0.0  ...   \n",
              "1                  229.0                   229.0                    0.0  ...   \n",
              "2                  229.0                   229.0                    0.0  ...   \n",
              "3                  247.0                   247.0                    0.0  ...   \n",
              "4                  229.0                   229.0                    0.0  ...   \n",
              "\n",
              "   Fwd Seg Size Min  Active Mean  Active Std  Active Max  Active Min  \\\n",
              "0                20          0.0         0.0         0.0         0.0   \n",
              "1                 0          0.0         0.0         0.0         0.0   \n",
              "2                20          0.0         0.0         0.0         0.0   \n",
              "3                20          0.0         0.0         0.0         0.0   \n",
              "4                20          0.0         0.0         0.0         0.0   \n",
              "\n",
              "   Idle Mean  Idle Std  Idle Max  Idle Min  Label  \n",
              "0        0.0       0.0       0.0       0.0      5  \n",
              "1        0.0       0.0       0.0       0.0      5  \n",
              "2        0.0       0.0       0.0       0.0      5  \n",
              "3        0.0       0.0       0.0       0.0      5  \n",
              "4        0.0       0.0       0.0       0.0      5  \n",
              "\n",
              "[5 rows x 78 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0219c30e-456e-4e85-aa21-b46c6644ce2c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Fwd Packets Length Total</th>\n",
              "      <th>Bwd Packets Length Total</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>...</th>\n",
              "      <th>Fwd Seg Size Min</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>422.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>494.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 78 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0219c30e-456e-4e85-aa21-b46c6644ce2c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0219c30e-456e-4e85-aa21-b46c6644ce2c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0219c30e-456e-4e85-aa21-b46c6644ce2c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cdd3a967-ca05-4f29-8843-8aefdbbbb5ae\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cdd3a967-ca05-4f29-8843-8aefdbbbb5ae')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cdd3a967-ca05-4f29-8843-8aefdbbbb5ae button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_encoder.classes_)"
      ],
      "metadata": {
        "id": "ZFjy9aA1dTxA",
        "outputId": "d0110e34-ef27-4ccd-dfdc-a65c0f46add8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'label_encoder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0d0421a50f11>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'label_encoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyxdjUHZ_yjW",
        "outputId": "d84456c3-b3bc-423d-bbb4-9736195fecf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape after loading: (431371, 78)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5393/5393 - 16s - 3ms/step - accuracy: 0.5351 - loss: 1.4540 - val_accuracy: 0.6937 - val_loss: 1.0608\n",
            "Epoch 2/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.6952 - loss: 1.0721 - val_accuracy: 0.6992 - val_loss: 0.9507\n",
            "Epoch 3/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.6995 - loss: 1.0203 - val_accuracy: 0.7010 - val_loss: 0.9217\n",
            "Epoch 4/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7044 - loss: 1.0108 - val_accuracy: 0.7047 - val_loss: 0.8899\n",
            "Epoch 5/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7382 - loss: 0.8556 - val_accuracy: 0.7726 - val_loss: 0.5964\n",
            "Epoch 6/100\n",
            "5393/5393 - 19s - 3ms/step - accuracy: 0.7755 - loss: 0.6698 - val_accuracy: 0.7801 - val_loss: 0.5591\n",
            "Epoch 7/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7798 - loss: 0.6397 - val_accuracy: 0.7802 - val_loss: 0.5466\n",
            "Epoch 8/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7814 - loss: 0.6247 - val_accuracy: 0.7798 - val_loss: 0.5398\n",
            "Epoch 9/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7823 - loss: 0.6132 - val_accuracy: 0.7826 - val_loss: 0.5348\n",
            "Epoch 10/100\n",
            "5393/5393 - 20s - 4ms/step - accuracy: 0.7831 - loss: 0.6043 - val_accuracy: 0.7826 - val_loss: 0.5326\n",
            "Epoch 11/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7835 - loss: 0.5970 - val_accuracy: 0.7820 - val_loss: 0.5299\n",
            "Epoch 12/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7839 - loss: 0.5920 - val_accuracy: 0.7832 - val_loss: 0.5276\n",
            "Epoch 13/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7843 - loss: 0.5888 - val_accuracy: 0.7878 - val_loss: 0.5256\n",
            "Epoch 14/100\n",
            "5393/5393 - 18s - 3ms/step - accuracy: 0.7846 - loss: 0.5838 - val_accuracy: 0.7836 - val_loss: 0.5246\n",
            "Epoch 15/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7847 - loss: 0.5807 - val_accuracy: 0.7854 - val_loss: 0.5233\n",
            "Epoch 16/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7854 - loss: 0.5777 - val_accuracy: 0.7838 - val_loss: 0.5224\n",
            "Epoch 17/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7859 - loss: 0.5739 - val_accuracy: 0.7873 - val_loss: 0.5211\n",
            "Epoch 18/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7868 - loss: 0.5738 - val_accuracy: 0.7855 - val_loss: 0.5183\n",
            "Epoch 19/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7872 - loss: 0.5684 - val_accuracy: 0.7888 - val_loss: 0.5172\n",
            "Epoch 20/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7878 - loss: 0.5698 - val_accuracy: 0.7883 - val_loss: 0.5149\n",
            "Epoch 21/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7880 - loss: 0.5656 - val_accuracy: 0.7885 - val_loss: 0.5144\n",
            "Epoch 22/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7884 - loss: 0.5646 - val_accuracy: 0.7889 - val_loss: 0.5131\n",
            "Epoch 23/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7884 - loss: 0.5617 - val_accuracy: 0.7880 - val_loss: 0.5133\n",
            "Epoch 24/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7882 - loss: 0.5577 - val_accuracy: 0.7894 - val_loss: 0.5119\n",
            "Epoch 25/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7888 - loss: 0.5639 - val_accuracy: 0.7886 - val_loss: 0.5090\n",
            "Epoch 26/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7886 - loss: 0.5581 - val_accuracy: 0.7881 - val_loss: 0.5108\n",
            "Epoch 27/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7887 - loss: 0.5546 - val_accuracy: 0.7870 - val_loss: 0.5094\n",
            "Epoch 28/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7888 - loss: 0.5521 - val_accuracy: 0.7906 - val_loss: 0.5108\n",
            "Epoch 29/100\n",
            "5393/5393 - 12s - 2ms/step - accuracy: 0.7886 - loss: 0.5493 - val_accuracy: 0.7872 - val_loss: 0.5087\n",
            "Epoch 30/100\n",
            "5393/5393 - 19s - 4ms/step - accuracy: 0.7889 - loss: 0.5478 - val_accuracy: 0.7873 - val_loss: 0.5074\n",
            "Epoch 31/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7889 - loss: 0.5456 - val_accuracy: 0.7865 - val_loss: 0.5079\n",
            "Epoch 32/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7886 - loss: 0.5420 - val_accuracy: 0.7875 - val_loss: 0.5070\n",
            "Epoch 33/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7887 - loss: 0.5387 - val_accuracy: 0.7870 - val_loss: 0.5078\n",
            "Epoch 34/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7889 - loss: 0.5369 - val_accuracy: 0.7877 - val_loss: 0.5064\n",
            "Epoch 35/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7890 - loss: 0.5350 - val_accuracy: 0.7880 - val_loss: 0.5059\n",
            "Epoch 36/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7888 - loss: 0.5313 - val_accuracy: 0.7882 - val_loss: 0.5058\n",
            "Epoch 37/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7894 - loss: 0.5314 - val_accuracy: 0.7868 - val_loss: 0.5066\n",
            "Epoch 38/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7889 - loss: 0.5272 - val_accuracy: 0.7890 - val_loss: 0.5043\n",
            "Epoch 39/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7890 - loss: 0.5255 - val_accuracy: 0.7886 - val_loss: 0.5040\n",
            "Epoch 40/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7890 - loss: 0.5214 - val_accuracy: 0.7895 - val_loss: 0.5038\n",
            "Epoch 41/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7892 - loss: 0.5184 - val_accuracy: 0.7882 - val_loss: 0.5050\n",
            "Epoch 42/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7890 - loss: 0.5144 - val_accuracy: 0.7891 - val_loss: 0.5027\n",
            "Epoch 43/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7891 - loss: 0.5158 - val_accuracy: 0.7912 - val_loss: 0.5031\n",
            "Epoch 44/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7894 - loss: 0.5128 - val_accuracy: 0.7891 - val_loss: 0.5027\n",
            "Epoch 45/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7891 - loss: 0.5104 - val_accuracy: 0.7884 - val_loss: 0.5034\n",
            "Epoch 46/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7894 - loss: 0.5205 - val_accuracy: 0.7881 - val_loss: 0.5003\n",
            "Epoch 47/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7895 - loss: 0.5157 - val_accuracy: 0.7880 - val_loss: 0.5005\n",
            "Epoch 48/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7898 - loss: 0.5121 - val_accuracy: 0.7898 - val_loss: 0.5009\n",
            "Epoch 49/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7899 - loss: 0.5132 - val_accuracy: 0.7886 - val_loss: 0.5004\n",
            "Epoch 50/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7899 - loss: 0.5099 - val_accuracy: 0.7900 - val_loss: 0.5001\n",
            "Epoch 51/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7898 - loss: 0.5153 - val_accuracy: 0.7890 - val_loss: 0.4993\n",
            "Epoch 52/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7902 - loss: 0.5094 - val_accuracy: 0.7872 - val_loss: 0.5010\n",
            "Epoch 53/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7896 - loss: 0.5078 - val_accuracy: 0.7887 - val_loss: 0.5002\n",
            "Epoch 54/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7897 - loss: 0.5085 - val_accuracy: 0.7894 - val_loss: 0.4997\n",
            "Epoch 55/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7898 - loss: 0.5083 - val_accuracy: 0.7912 - val_loss: 0.4998\n",
            "Epoch 56/100\n",
            "5393/5393 - 19s - 4ms/step - accuracy: 0.7899 - loss: 0.5096 - val_accuracy: 0.7921 - val_loss: 0.4998\n",
            "Epoch 57/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7898 - loss: 0.5076 - val_accuracy: 0.7897 - val_loss: 0.4991\n",
            "Epoch 58/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7900 - loss: 0.5058 - val_accuracy: 0.7884 - val_loss: 0.4995\n",
            "Epoch 59/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7899 - loss: 0.5065 - val_accuracy: 0.7883 - val_loss: 0.4993\n",
            "Epoch 60/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7900 - loss: 0.5065 - val_accuracy: 0.7894 - val_loss: 0.4984\n",
            "Epoch 61/100\n",
            "5393/5393 - 20s - 4ms/step - accuracy: 0.7902 - loss: 0.5058 - val_accuracy: 0.7908 - val_loss: 0.4985\n",
            "Epoch 62/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7904 - loss: 0.5133 - val_accuracy: 0.7913 - val_loss: 0.4993\n",
            "Epoch 63/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7900 - loss: 0.5103 - val_accuracy: 0.7909 - val_loss: 0.4980\n",
            "Epoch 64/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7902 - loss: 0.5089 - val_accuracy: 0.7912 - val_loss: 0.4982\n",
            "Epoch 65/100\n",
            "5393/5393 - 19s - 4ms/step - accuracy: 0.7904 - loss: 0.5147 - val_accuracy: 0.7881 - val_loss: 0.4979\n",
            "Epoch 66/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7902 - loss: 0.5086 - val_accuracy: 0.7917 - val_loss: 0.4976\n",
            "Epoch 67/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7904 - loss: 0.5077 - val_accuracy: 0.7895 - val_loss: 0.4977\n",
            "Epoch 68/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7904 - loss: 0.5060 - val_accuracy: 0.7899 - val_loss: 0.4980\n",
            "Epoch 69/100\n",
            "5393/5393 - 20s - 4ms/step - accuracy: 0.7903 - loss: 0.5133 - val_accuracy: 0.7904 - val_loss: 0.4973\n",
            "Epoch 70/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7905 - loss: 0.5090 - val_accuracy: 0.7909 - val_loss: 0.4982\n",
            "Epoch 71/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7902 - loss: 0.5067 - val_accuracy: 0.7891 - val_loss: 0.4978\n",
            "Epoch 72/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7903 - loss: 0.5100 - val_accuracy: 0.7894 - val_loss: 0.4976\n",
            "Epoch 73/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7905 - loss: 0.5070 - val_accuracy: 0.7871 - val_loss: 0.4992\n",
            "Epoch 74/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7905 - loss: 0.5075 - val_accuracy: 0.7889 - val_loss: 0.4977\n",
            "Epoch 75/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7904 - loss: 0.5063 - val_accuracy: 0.7931 - val_loss: 0.4976\n",
            "Epoch 76/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7907 - loss: 0.5129 - val_accuracy: 0.7895 - val_loss: 0.4971\n",
            "Epoch 77/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7909 - loss: 0.5111 - val_accuracy: 0.7902 - val_loss: 0.4973\n",
            "Epoch 78/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7904 - loss: 0.5081 - val_accuracy: 0.7918 - val_loss: 0.4970\n",
            "Epoch 79/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7907 - loss: 0.5108 - val_accuracy: 0.7908 - val_loss: 0.4966\n",
            "Epoch 80/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7906 - loss: 0.5064 - val_accuracy: 0.7899 - val_loss: 0.4971\n",
            "Epoch 81/100\n",
            "5393/5393 - 19s - 4ms/step - accuracy: 0.7906 - loss: 0.5079 - val_accuracy: 0.7897 - val_loss: 0.4982\n",
            "Epoch 82/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7904 - loss: 0.5060 - val_accuracy: 0.7902 - val_loss: 0.4970\n",
            "Epoch 83/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7910 - loss: 0.5074 - val_accuracy: 0.7899 - val_loss: 0.4968\n",
            "Epoch 84/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7912 - loss: 0.5115 - val_accuracy: 0.7930 - val_loss: 0.4984\n",
            "Epoch 85/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7907 - loss: 0.5084 - val_accuracy: 0.7891 - val_loss: 0.4978\n",
            "Epoch 86/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7903 - loss: 0.5058 - val_accuracy: 0.7912 - val_loss: 0.4973\n",
            "Epoch 87/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7906 - loss: 0.5057 - val_accuracy: 0.7907 - val_loss: 0.4973\n",
            "Epoch 88/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7908 - loss: 0.5037 - val_accuracy: 0.7925 - val_loss: 0.4983\n",
            "Epoch 89/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7910 - loss: 0.5105 - val_accuracy: 0.7903 - val_loss: 0.4963\n",
            "Epoch 90/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7914 - loss: 0.5086 - val_accuracy: 0.7914 - val_loss: 0.4966\n",
            "Epoch 91/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7913 - loss: 0.5100 - val_accuracy: 0.7925 - val_loss: 0.4965\n",
            "Epoch 92/100\n",
            "5393/5393 - 10s - 2ms/step - accuracy: 0.7910 - loss: 0.5063 - val_accuracy: 0.7921 - val_loss: 0.4964\n",
            "Epoch 93/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7911 - loss: 0.5088 - val_accuracy: 0.7915 - val_loss: 0.4963\n",
            "Epoch 94/100\n",
            "5393/5393 - 20s - 4ms/step - accuracy: 0.7915 - loss: 0.5050 - val_accuracy: 0.7917 - val_loss: 0.4969\n",
            "Epoch 95/100\n",
            "5393/5393 - 19s - 3ms/step - accuracy: 0.7912 - loss: 0.5078 - val_accuracy: 0.7936 - val_loss: 0.4975\n",
            "Epoch 96/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7913 - loss: 0.5056 - val_accuracy: 0.7918 - val_loss: 0.4968\n",
            "Epoch 97/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7911 - loss: 0.5045 - val_accuracy: 0.7910 - val_loss: 0.4968\n",
            "Epoch 98/100\n",
            "5393/5393 - 20s - 4ms/step - accuracy: 0.7914 - loss: 0.5055 - val_accuracy: 0.7900 - val_loss: 0.4971\n",
            "Epoch 99/100\n",
            "5393/5393 - 9s - 2ms/step - accuracy: 0.7913 - loss: 0.5031 - val_accuracy: 0.7889 - val_loss: 0.4970\n",
            "Epoch 100/100\n",
            "5393/5393 - 11s - 2ms/step - accuracy: 0.7916 - loss: 0.5123 - val_accuracy: 0.7881 - val_loss: 0.4964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 78.81%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step\n",
            "Predicted DDoS Attack Type: DrDoS_NTP\n",
            "Weights and biases in IEEE 754 format saved in 'WeightsAndBiases_IEEE754.txt'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "# Selected features based on the data you shared\n",
        "selected_features = [\n",
        "    'Protocol',\n",
        "    'Flow Duration',\n",
        "    'Total Fwd Packets',\n",
        "    'Total Backward Packets',\n",
        "    'Fwd Packets Length Total',\n",
        "    'Bwd Packets Length Total',\n",
        "    'Fwd Packet Length Max',\n",
        "    'Fwd Packet Length Min',\n",
        "    'Fwd Packet Length Mean',\n",
        "    'Fwd Packet Length Std'\n",
        "]\n",
        "\n",
        "target_column = 'Label'\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_parquet(\"/content/data.parquet\")\n",
        "print(\"Data shape after loading:\", data.shape)\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "data[target_column] = label_encoder.fit_transform(data[target_column])\n",
        "\n",
        "# Split features and target\n",
        "X = data[selected_features]\n",
        "y = data[target_column]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential([\n",
        "    Dense(2, activation='relu', input_dim=X_train.shape[1]),\n",
        "\n",
        "    Dense(2, activation='relu'),\n",
        "\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')  # Multi-class classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train, validation_data=(X_test, y_test),\n",
        "    epochs=100, batch_size=64, verbose=2\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"ddos_attack_type_model.h5\")\n",
        "\n",
        "# Example usage: Predict the attack type\n",
        "sample_prediction = model.predict(X_test[:1])\n",
        "predicted_class = label_encoder.inverse_transform([sample_prediction.argmax()])\n",
        "print(f\"Predicted DDoS Attack Type: {predicted_class[0]}\")\n",
        "\n",
        "import struct\n",
        "import numpy as np\n",
        "\n",
        "def float_to_ieee754(value):\n",
        "    \"\"\"Convert a float to IEEE 754 single-precision (32-bit) representation.\"\"\"\n",
        "    packed = struct.pack('>f', value)  # Pack as big-endian float\n",
        "    ieee754 = struct.unpack('>I', packed)[0]  # Unpack as unsigned int\n",
        "    return f\"0x{ieee754:08X}\"  # Return as hexadecimal string\n",
        "\n",
        "# Extract weights and biases from the trained model\n",
        "with open(\"WeightsAndBiases_IEEE754.txt\", \"w\") as weights_file:\n",
        "    weights_file.write(\"Weights and Biases of the Trained Neural Network in IEEE 754 Format:\\n\\n\")\n",
        "\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        weights_file.write(f\"Layer {i + 1}: {layer.name}\\n\")\n",
        "\n",
        "        if hasattr(layer, \"get_weights\"):\n",
        "            weights = layer.get_weights()\n",
        "\n",
        "            if len(weights) > 0:\n",
        "                weights_file.write(\"Weights (IEEE 754):\\n\")\n",
        "                ieee_weights = np.vectorize(float_to_ieee754)(weights[0].flatten())\n",
        "                weights_str = \", \".join(ieee_weights)\n",
        "                weights_file.write(\"{\" + weights_str + \"},\\n\\n\")\n",
        "\n",
        "            if len(weights) > 1:\n",
        "                weights_file.write(\"Biases (IEEE 754):\\n\")\n",
        "                ieee_biases = np.vectorize(float_to_ieee754)(weights[1].flatten())\n",
        "                biases_str = \", \".join(ieee_biases)\n",
        "                weights_file.write(\"{\" + biases_str + \"},\\n\\n\")\n",
        "        else:\n",
        "            weights_file.write(\"No weights available for this layer.\\n\\n\")\n",
        "\n",
        "print(\"Weights and biases in IEEE 754 format saved in 'WeightsAndBiases_IEEE754.txt'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract weights and biases from the trained model\n",
        "with open(\"WeightsAndBiases.txt\", \"w\") as weights_file:\n",
        "    weights_file.write(\"Weights and Biases of the Trained Neural Network:\\n\\n\")\n",
        "\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        weights_file.write(f\"Layer {i + 1}: {layer.name}\\n\")\n",
        "\n",
        "        if hasattr(layer, \"get_weights\"):\n",
        "            weights = layer.get_weights()\n",
        "\n",
        "            if len(weights) > 0:\n",
        "                # Write the weights\n",
        "                weights_file.write(\"Weights:\\n\")\n",
        "                scaled_weights = (weights[0] * 1e5).astype(int)  # Scaling by 1e5\n",
        "                weights_str = np.array2string(\n",
        "                    scaled_weights, threshold=np.inf, precision=4, separator=\",\",\n",
        "                    suppress_small=True\n",
        "                ).replace('[', '{').replace(']', '}')\n",
        "                weights_file.write(weights_str + \",\\n\\n\")\n",
        "\n",
        "            if len(weights) > 1:\n",
        "                # Write the biases\n",
        "                weights_file.write(\"Biases:\\n\")\n",
        "                scaled_biases = (weights[1] * 1e5).astype(int)  # Scaling by 1e5\n",
        "                biases_str = np.array2string(\n",
        "                    scaled_biases, threshold=np.inf, precision=4, separator=\",\",\n",
        "                    suppress_small=True\n",
        "                ).replace('[', '{').replace(']', '}')\n",
        "                weights_file.write(biases_str + \",\\n\\n\")\n",
        "        else:\n",
        "            weights_file.write(\"No weights available for this layer.\\n\\n\")\n",
        "\n",
        "print(\"Weights and biases saved in 'WeightsAndBiases.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdPsHfK9S0ly",
        "outputId": "ed037690-05c5-4c11-a7b4-952d0d72d6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights and biases saved in 'WeightsAndBiases.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Define the same model architecture\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_dim=10),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(18, activation='softmax')  # Assuming 18 classes\n",
        "])\n",
        "\n",
        "# Initialize lists for weights and biases\n",
        "weights = []\n",
        "biases = []\n",
        "current_block = []\n",
        "\n",
        "# Parse weights and biases from the file\n",
        "with open(\"WeightsAndBiases.txt\", \"r\") as file:\n",
        "    for line in file:\n",
        "        if \"Weights:\" in line or \"Biases:\" in line:\n",
        "            # Save the current block (if it's non-empty)\n",
        "            if current_block:\n",
        "                parsed_block = np.array(current_block)  # Convert to NumPy array\n",
        "                if \"Weights\" in line:  # Last processed block was weights\n",
        "                    weights.append(parsed_block)\n",
        "                else:  # Last processed block was biases\n",
        "                    biases.append(parsed_block)\n",
        "            current_block = []  # Reset for the next block\n",
        "        elif \"{\" in line and \"}\" in line:\n",
        "            # Extract and parse the values\n",
        "            values = line.strip(\"{} \\n\").split(\",\")\n",
        "            current_block.append([int(v.strip()) / 1e5 for v in values])  # Scale back\n",
        "    # Handle the last block\n",
        "    if current_block:\n",
        "        parsed_block = np.array(current_block)\n",
        "        if \"Weights\" in line:  # If the last block was weights\n",
        "            weights.append(parsed_block)\n",
        "        else:  # If the last block was biases\n",
        "            biases.append(parsed_block)\n",
        "\n",
        "# Verify weights and biases counts\n",
        "print(f\"Number of weight sets: {len(weights)}\")\n",
        "print(f\"Number of bias sets: {len(biases)}\")\n",
        "\n",
        "# Set the weights and biases into the model\n",
        "for i, layer in enumerate(model.layers):\n",
        "    if hasattr(layer, \"set_weights\"):\n",
        "        try:\n",
        "            # Ensure the correct shape by transposing weight matrices as needed\n",
        "            weight = np.array(weights[i])\n",
        "            bias = np.array(biases[i]).flatten()  # Biases must be 1D\n",
        "            layer.set_weights([weight, bias])\n",
        "            print(f\"Set weights and biases for layer {layer.name}\")\n",
        "        except IndexError:\n",
        "            print(f\"Layer {layer.name} does not have weights/biases in the file.\")\n",
        "        except ValueError as e:\n",
        "            print(f\"Shape mismatch for layer {layer.name}: {e}\")\n",
        "\n",
        "print(\"Weights and biases loaded into the model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW4zy0NtRnye",
        "outputId": "126b7bb5-ef36-4bc7-ff6a-2ed56c0012dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of weight sets: 0\n",
            "Number of bias sets: 0\n",
            "Layer dense_18 does not have weights/biases in the file.\n",
            "Layer dropout_12 does not have weights/biases in the file.\n",
            "Layer dense_19 does not have weights/biases in the file.\n",
            "Layer dropout_13 does not have weights/biases in the file.\n",
            "Layer dense_20 does not have weights/biases in the file.\n",
            "Weights and biases loaded into the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = [\n",
        "    'Protocol',\n",
        "    'Flow Duration',\n",
        "    'Total Fwd Packets',\n",
        "    'Total Backward Packets',\n",
        "    'Fwd Packets Length Total',\n",
        "    'Bwd Packets Length Total',\n",
        "    'Fwd Packet Length Max',\n",
        "    'Fwd Packet Length Min',\n",
        "    'Fwd Packet Length Mean',\n",
        "    'Fwd Packet Length Std'\n",
        "]\n",
        "\n",
        "# Extract the first five rows of the selected features\n",
        "# subset = data[selected_features].head()\n",
        "\n",
        "# Convert to IEEE 754 format\n",
        "def to_ieee754(value):\n",
        "    if isinstance(value, (float, int)):\n",
        "        return hex(struct.unpack('>I', struct.pack('>f', float(value)))[0])\n",
        "    return None  # Handle non-numeric values if needed\n",
        "\n",
        "# Apply conversion to each element in the DataFrame\n",
        "ieee754_converted = data.map(to_ieee754)\n",
        "\n",
        "print(ieee754_converted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcpSR2y7O3Lh",
        "outputId": "89c4874e-8bc4-4d69-aac4-bbd46e2446b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Protocol Flow Duration Total Fwd Packets Total Backward Packets  \\\n",
            "0  0x3f800000    0x4640e400        0x41200000             0x40a00000   \n",
            "1  0x40c00000    0x47849900        0x41a00000             0x41200000   \n",
            "2  0x41880000    0x46b74000        0x41f00000             0x41700000   \n",
            "3  0x3f800000    0x479a1a80        0x42200000             0x41a00000   \n",
            "4  0x40c00000    0x47070700        0x42480000             0x41c80000   \n",
            "\n",
            "  Fwd Packets Length Total Bwd Packets Length Total Fwd Packet Length Max  \\\n",
            "0               0x42c90000               0x424a0000            0x42720000   \n",
            "1               0x4348199a               0x42c86666            0x428d999a   \n",
            "2               0x4396199a               0x43166666            0x42a1cccd   \n",
            "3               0x43c86666               0x4348999a            0x42b40000   \n",
            "4               0x43fa7333               0x437ab333            0x42c83333   \n",
            "\n",
            "  Fwd Packet Length Min Fwd Packet Length Mean Fwd Packet Length Std  \n",
            "0            0x3f99999a             0x41f0cccd            0x40a33333  \n",
            "1            0x40133333             0x420ccccd            0x40c66666  \n",
            "2            0x4059999a             0x42213333            0x40e9999a  \n",
            "3            0x40900000             0x4235999a            0x41066666  \n",
            "4            0x40b33333             0x424a0000            0x41180000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# |\n",
        "import numpy as np\n",
        "\n",
        "# Adjust print options to avoid truncation\n",
        "np.set_printoptions(suppress=True, precision=4, linewidth=200, threshold=np.inf)\n",
        "\n",
        "print(weights)"
      ],
      "metadata": {
        "id": "i2cjZB1I0bO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3043e46-2b8a-4b44-887c-7830fb719e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[-0.3491,  0.0369,  0.0865,  0.0217,  0.1834, -0.1087,  0.0864, -0.5011,  0.0845, -0.0332, -0.782 , -0.4631, -0.3584,  0.0088, -0.4618, -0.7453, -0.5147, -0.2046],\n",
            "       [-1.0478, -0.1065, -0.5998,  0.1845,  0.1221, -0.3713, -0.4991,  0.0164, -0.2199,  0.1819, -1.0469, -0.3704, -0.1713,  0.0933,  0.0514, -0.1899, -0.2513, -0.618 ],\n",
            "       [-0.4182,  0.1984, -0.8307, -0.2332,  0.2469, -1.144 , -0.8916,  0.2132, -0.3485, -0.3525, -0.7497, -1.3718, -0.2654, -0.3566,  0.1953,  0.2952, -0.0862, -0.0561],\n",
            "       [-0.3866, -0.0419,  0.0589, -0.3247,  0.5423, -0.1406, -0.4721, -0.5807,  0.1348, -0.3308, -0.4544, -0.0895, -0.324 ,  0.6479, -0.5728, -1.2683, -0.1213,  0.0251],\n",
            "       [-0.3987, -0.1767, -0.3523, -0.4398,  0.8166, -0.0384, -0.641 , -0.4884, -0.6797, -0.3891, -0.5669, -0.0301, -0.5367,  0.77  , -0.5711, -0.9661, -0.4149, -0.18  ],\n",
            "       [-0.4154,  0.0381, -0.8283, -0.6211,  0.1539,  0.192 , -0.1835,  0.0721,  0.0607, -0.5627,  0.1588,  0.1666, -0.1219, -1.3176,  0.0834,  0.2617, -0.1969, -0.5379],\n",
            "       [ 0.0841,  0.0964,  0.0969,  0.0199, -1.2273, -0.2813,  0.0873, -0.0432,  0.1101,  0.0559, -0.3974, -0.386 , -0.374 ,  0.1565, -0.1063, -0.3684, -0.2826, -0.4985],\n",
            "       [ 0.1604,  0.1923,  0.1449,  0.1427, -0.3042, -0.0164,  0.2038, -0.0844,  0.1927,  0.1309,  0.1114,  0.091 , -0.2876, -1.0213, -0.0853, -0.2191, -0.1731, -0.6032],\n",
            "       [-0.0468,  0.0774,  0.0697,  0.0059, -1.0134, -0.206 ,  0.071 , -0.2282,  0.0727,  0.0086, -0.9022, -0.3038, -0.5275, -0.8834, -0.397 , -0.0468, -0.3938, -0.6409],\n",
            "       [-0.5089,  0.1157,  0.0877,  0.0912, -0.0613, -0.2753,  0.108 , -0.6675,  0.1553,  0.0632, -1.6282, -0.5157, -0.3993, -0.1301, -0.6888, -0.584 , -0.6978, -0.9262],\n",
            "       [-0.5643, -0.3983, -0.7067, -0.0564,  0.3369, -0.64  , -1.4172,  0.2386, -0.5259,  0.0858, -1.0484, -1.0154, -0.3688, -0.3259,  0.231 ,  0.1025, -0.439 , -0.363 ],\n",
            "       [-0.3841,  0.1271,  0.0776, -0.8391,  0.5022, -0.3285, -0.6516, -0.0599, -0.1403, -0.5632, -0.1021, -0.5862, -0.2733, -0.4674, -0.0949, -0.0304, -0.3864,  0.0671],\n",
            "       [ 0.0996, -0.0388, -0.0704, -0.3241,  0.0231, -0.2194,  0.0516,  0.0215, -0.027 , -0.3562, -0.2227, -0.1205, -0.7657, -0.1507, -0.0256, -0.2375, -0.5078, -0.3336],\n",
            "       [ 0.2709,  0.0243,  0.0429,  0.0016,  0.3086, -0.476 ,  0.0589, -0.0663,  0.0888,  0.0398, -0.5128,  0.2377, -0.0216, -0.1786, -0.0208, -0.3432, -0.3631,  0.0818],\n",
            "       [ 0.0131, -1.1119, -0.4314, -1.5275, -0.3716, -1.0038, -0.9857, -0.3292, -1.0325, -0.6893, -0.6782, -0.1659,  0.2566, -0.4159, -0.8973,  0.2551, -0.2039, -0.2448],\n",
            "       [ 0.2484, -0.3079, -0.4802, -1.141 , -0.3042, -0.0999, -0.1867,  0.1735, -0.6011, -0.7982, -0.0657, -0.15  ,  0.2802, -0.464 ,  0.2007,  0.083 , -1.0769, -0.5329],\n",
            "       [-0.4079, -0.4633, -0.208 , -0.1378, -0.2655, -0.6143, -0.8217, -0.2353, -0.8318, -0.1985, -0.3295, -0.9481,  0.3551, -0.6907, -0.5191,  0.2087, -0.4152, -0.4594],\n",
            "       [-0.653 , -1.4771, -0.5603, -0.3575, -0.291 , -0.7988, -0.9085, -0.7937, -1.001 , -0.6452, -0.3295, -1.3773,  0.3643, -0.5942, -0.4757,  0.5413, -0.4952, -0.356 ],\n",
            "       [-0.0974,  0.0112, -1.1394, -0.7164, -1.0074, -0.001 ,  0.0881,  0.2925, -0.6392, -0.9625,  0.1063,  0.1927,  0.2848, -0.9844,  0.342 ,  0.1123,  0.045 , -0.597 ],\n",
            "       [-0.4764,  0.111 ,  0.1159,  0.0123, -1.4159, -0.0092,  0.0691, -0.1112,  0.1067,  0.0037, -0.2827, -0.0748,  0.1888,  0.0516, -0.2041, -0.0713, -0.3156, -0.5219],\n",
            "       [ 0.1251, -0.3946, -0.2362, -0.4016, -0.1147, -0.7357, -0.4921, -0.1865, -0.5411, -0.87  , -0.9991, -0.0552, -0.0754, -0.1415, -0.3418, -0.2362, -0.3505, -0.0997],\n",
            "       [ 0.1611, -0.0518, -0.2706, -0.5723,  0.1241, -0.171 , -0.3209, -0.7274, -0.4743, -0.4443, -0.1036, -0.0819,  0.1521,  0.111 , -0.9814,  0.1449, -0.3765,  0.0025],\n",
            "       [ 0.1193,  0.0153, -0.4145, -0.4483, -0.1848,  0.0701,  0.0248, -0.1805, -0.1466, -0.2056,  0.1101,  0.1606,  0.0946, -0.6303, -0.1447,  0.0069, -0.3294, -0.1767],\n",
            "       [-0.1054,  0.1362,  0.0048,  0.0617,  0.5781, -0.2771, -0.1118, -1.0157,  0.1401, -0.0876, -0.3669, -0.1191,  0.3399, -0.6465, -0.7707, -0.3356, -0.1491, -0.4751],\n",
            "       [-0.0338,  0.2074,  0.0464, -0.4021,  0.859 , -0.1348, -0.0609, -0.4588,  0.0793, -0.4273, -0.403 , -0.4802, -0.0045, -0.9007, -0.6008, -0.8106, -0.0614, -0.0228],\n",
            "       [-0.8861,  0.0046, -0.3936,  0.1293,  0.2053, -0.242 , -0.5422,  0.1756, -0.2869,  0.1709, -0.446 , -0.4539,  0.0006,  0.0807,  0.2277,  0.191 , -0.0448, -0.5924],\n",
            "       [ 0.1109,  0.0033,  0.0522,  0.1857, -0.0371, -0.0375,  0.1227, -0.5385,  0.1097,  0.2936, -0.5085, -0.0208,  0.257 ,  0.219 , -0.2697,  0.2527, -0.011 , -0.2388],\n",
            "       [ 0.1724,  0.1814,  0.1939, -0.1376,  0.208 , -0.6684,  0.1533, -0.4751,  0.1638, -0.0788, -0.4492, -0.2859, -0.5641, -0.17  , -0.4762, -0.2239, -0.5103, -0.104 ],\n",
            "       [ 0.2305,  0.0168, -0.3561, -0.7485,  0.0993, -0.1224, -0.1742, -0.5072, -0.3522, -0.6479, -0.1298,  0.0527,  0.2221,  0.1971, -0.4101,  0.1323, -0.454 , -0.2879],\n",
            "       [-0.9766, -0.0722, -0.103 ,  0.2874, -0.3616,  0.0393, -0.2583,  0.0708, -0.2383,  0.3915,  0.0645,  0.1348, -0.4163,  0.3171,  0.1485,  0.0746, -0.2007, -0.6548],\n",
            "       [-0.8138, -0.5222, -1.6288,  0.1682,  0.4666, -0.2818, -1.2218,  0.2304, -1.4597,  0.2998, -0.953 , -0.8846, -0.4086, -1.017 ,  0.1846,  0.1981, -0.3193, -0.7378],\n",
            "       [-1.2597, -0.2429, -1.0155, -0.0909, -0.0478, -0.1178, -0.6409,  0.0889, -0.8248, -0.0735,  0.245 ,  0.1374, -0.321 ,  0.0277,  0.0981,  0.0017, -0.1402, -1.7606],\n",
            "       [ 0.3496,  0.0254,  0.1391, -0.2918,  0.2051, -0.3054,  0.1664, -0.1037,  0.1686, -0.3788, -0.1484, -0.1784,  0.2717, -0.1735, -0.1287, -0.1448, -0.2088, -0.0706],\n",
            "       [-0.1215, -0.2707, -0.6247,  0.1352,  0.4575, -0.2013, -0.7848, -0.1596, -0.6188,  0.2118, -0.37  , -0.2837, -0.139 ,  0.4379, -0.3532, -0.9346, -0.3107,  0.0053],\n",
            "       [ 0.2588, -0.2077, -0.4692, -0.3128,  0.1817, -0.383 , -0.4801,  0.1018, -0.258 , -0.0858, -0.2421, -0.0824,  0.249 ,  0.2445,  0.0527, -0.1472, -0.247 ,  0.1333],\n",
            "       [-0.6615, -0.6233, -0.0131, -0.7021, -0.0395, -0.9576, -0.3277,  0.3826, -0.2651, -0.6588, -0.6402, -0.6274, -0.5652, -0.6083,  0.4061,  0.4319, -0.4255, -0.3119],\n",
            "       [ 0.267 , -0.2233, -0.6199, -0.5936,  0.041 , -0.3406, -0.4855,  0.2962, -0.6709, -0.2286, -0.4609, -0.064 ,  0.1637,  0.3622,  0.2326, -0.0003, -0.3231,  0.1078],\n",
            "       [-0.9122, -0.5377, -0.9051, -0.0457,  0.319 , -0.288 , -1.1568,  0.3437, -1.4399, -0.0957, -0.37  , -0.5566, -0.0732, -0.9837,  0.3136,  0.3051, -0.1761, -0.303 ],\n",
            "       [ 0.3203, -0.1129, -0.1892, -0.4308, -0.4569, -0.1832,  0.1291, -0.3922, -0.0701, -0.3878,  0.2288,  0.2674,  0.1043,  0.5412, -0.3732, -1.2202, -0.0564,  0.1167],\n",
            "       [ 0.0181,  0.2764,  0.0217, -0.6612,  0.2367,  0.0286, -0.484 , -0.2133,  0.1452, -0.4883,  0.0278, -0.4097,  0.3494,  0.6657, -0.141 , -0.5447, -0.3614,  0.0975],\n",
            "       [-0.2786, -0.3664, -0.8526, -0.0434, -1.1093, -0.0423, -0.6598,  0.1759, -0.4901, -0.0629, -0.0811, -0.3185,  0.0492,  0.4316,  0.2025,  0.0365, -0.1244, -1.5892],\n",
            "       [-0.2103,  0.1051,  0.1186,  0.2173,  0.3726, -0.1169,  0.189 ,  0.0325,  0.1406,  0.3223, -1.0143, -0.5433, -0.148 , -1.1228,  0.0038,  0.151 , -0.2748, -0.2852],\n",
            "       [-0.9198, -0.0464, -0.0217,  0.0007, -1.3106, -0.3558,  0.0285,  0.2614, -0.0282,  0.    , -0.6705, -0.4802, -0.5337,  0.0773,  0.3152,  0.1931, -0.2001, -0.8213],\n",
            "       [ 0.218 ,  0.0702,  0.0292, -0.26  ,  0.0198,  0.1913,  0.0458, -0.5669,  0.1131, -0.3019, -0.0609, -0.025 ,  0.2037, -0.267 , -0.8927,  0.1806, -0.7241, -0.0586],\n",
            "       [ 0.2332, -0.7157, -0.0119, -0.5358, -0.7691, -0.2517, -0.3386, -0.4064,  0.0071, -0.6619, -0.4113, -0.6174, -0.4373,  1.6453, -0.6201, -0.5068, -0.9335, -0.2388],\n",
            "       [-0.7039, -0.9739, -0.8502, -1.061 , -0.4891, -0.7471, -1.2134, -1.1265, -0.7464, -1.0985, -0.8647, -2.2535,  0.2674, -0.7629, -0.8843,  0.3955, -0.7864, -0.7229],\n",
            "       [-0.1827, -0.8819, -0.5429, -0.5808, -0.5187, -0.4257, -1.2043, -0.0468, -0.3949, -0.5738, -0.4308, -0.2882,  0.1637,  0.7159,  0.2064,  0.3437, -0.3907, -0.1566],\n",
            "       [-0.4021, -1.6932, -0.5315, -0.2573, -0.7506, -0.4868, -1.2072, -0.6355, -0.6342, -0.6201, -0.8998, -0.9707,  0.4264, -0.8674, -0.854 , -0.0672, -0.5719,  0.0639],\n",
            "       [-0.4315,  0.1496, -0.4313,  0.2502, -0.4021,  0.2863, -0.1478,  0.2737, -0.4256,  0.2488,  0.1594,  0.4174, -0.5489, -1.5215,  0.3078,  0.2357, -0.0514, -1.0861],\n",
            "       [ 0.3939,  0.0449,  0.0393, -0.1824, -1.1708, -0.3086,  0.0831,  0.2027, -0.2381, -0.0828, -0.2844, -0.0624, -0.2079,  0.8362,  0.1826,  0.0475, -0.1459, -0.925 ],\n",
            "       [ 0.0592,  0.0856, -0.3163,  0.172 ,  0.1562,  0.3183,  0.0451,  0.2183, -0.2146,  0.132 ,  0.2739,  0.1357, -0.6469, -1.1537,  0.2515,  0.1382, -0.1784, -0.8519],\n",
            "       [ 0.2366, -0.4383, -0.515 ,  0.2212,  0.0199, -0.3578, -0.4453, -0.2133, -0.4782,  0.1798, -0.5704, -0.2471,  0.194 ,  0.0042, -0.1134,  0.2021, -0.2492, -0.3416],\n",
            "       [ 0.3163,  0.219 , -0.1589,  0.2442, -0.1758,  0.0819, -0.0625, -0.1812, -0.2552,  0.3211,  0.0396, -0.0102, -0.5903,  0.3946, -0.101 , -0.883 ,  0.2097, -0.4722],\n",
            "       [ 0.2554, -0.1314, -0.2336, -0.5078,  0.0381, -0.6954, -0.3817, -0.67  , -0.2464, -0.4894, -0.5169, -0.0294,  0.2057, -0.0427, -1.0267,  0.0888, -0.2935,  0.0696],\n",
            "       [-0.6458,  0.1803, -0.2052,  0.1823,  0.4893,  0.0313, -0.2143, -0.2596, -0.2536,  0.2943, -0.7891, -0.3515, -0.225 ,  0.2434, -0.3098, -0.9224, -0.2175, -0.1203],\n",
            "       [-0.5632,  0.0195, -0.1459, -0.2862, -0.1736, -0.3749,  0.132 , -0.7024, -0.0681, -0.358 , -0.6796, -0.605 ,  0.1572,  0.2318, -0.6713,  0.4698, -1.1413, -0.8546],\n",
            "       [ 0.2243,  0.0719, -0.2288, -0.7749, -0.3797,  0.2655,  0.1059, -0.1466, -0.2417, -0.9376,  0.2146,  0.1661,  0.2026, -0.5135, -0.1135, -0.2515, -0.3918, -0.368 ],\n",
            "       [-0.2482,  0.2752, -0.3728, -0.6673,  0.4466, -0.3439, -0.1905,  0.0121, -0.348 , -0.7605, -0.2789, -0.4217, -0.4624,  0.6829, -0.0603, -0.3812, -0.4184, -0.4139],\n",
            "       [ 0.309 , -0.2532, -0.2101, -0.0536, -0.1188,  0.1237, -0.062 ,  0.0433, -0.0869, -0.2183, -0.1726, -0.2541,  0.0582, -0.3829,  0.0564, -0.0786, -0.1183, -0.0857],\n",
            "       [ 0.0497,  0.0836,  0.2191, -0.3807, -0.1316, -0.0198,  0.0071, -0.0581,  0.0596, -0.4518,  0.0388, -0.1249, -0.7863, -0.1394, -0.0297, -0.3548, -0.0991, -0.4646],\n",
            "       [ 0.3078, -0.1276, -0.3516, -0.5601,  0.103 , -0.2278, -0.2659, -0.5821, -0.3102, -0.2851, -0.2376, -0.0843,  0.2616,  0.0517, -0.4687,  0.1966, -0.2043, -0.1086],\n",
            "       [ 0.2864, -0.0012,  0.0108,  0.1982, -0.1455, -0.0946,  0.0743,  0.129 ,  0.0679,  0.1055, -0.0044, -0.0431, -0.295 ,  0.0334,  0.1502, -0.1311, -0.0993, -0.127 ],\n",
            "       [ 0.0715,  0.0926, -0.4545,  0.1838, -0.2621,  0.3703, -0.1557, -0.2136, -0.5312,  0.3121,  0.1453, -0.0237, -0.8202,  0.2188, -0.0639, -0.7266, -0.2407, -1.0832],\n",
            "       [ 0.2233,  0.0795, -0.1174, -0.4097, -0.0262,  0.0564, -0.119 ,  0.0141, -0.0047, -0.1842, -0.0483, -0.0199,  0.1482, -0.442 ,  0.0335, -0.081 , -0.2887, -0.1178]], dtype=float32), array([ 0.1711, -0.2778, -0.2793,  0.0952, -0.1369, -0.0791, -0.0166, -0.0275, -0.242 ,  0.1559, -0.0111, -0.1092,  0.0883, -0.037 ,  0.1866, -0.1868, -0.0946, -0.3224], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"WeightsAndBiases.txt\", \"w\") as weights_file:\n",
        "    weights_file.write(\"Weights and Biases of the Trained Neural Network:\\n\\n\")\n",
        "\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if hasattr(layer, \"weights\"):  # Check if the layer has weights\n",
        "            weights = layer.get_weights()\n",
        "\n",
        "            if weights:\n",
        "                weights_file.write(f\"Layer {i + 1}: {layer.name}\\n\")\n",
        "\n",
        "                if len(weights) > 0:\n",
        "                    weights_file.write(\"Weights:\\n\")\n",
        "                    weights_file.write(str(weights[0]))\n",
        "                    weights_file.write(\"\\n\\n\")\n",
        "\n",
        "                if len(weights) > 1:\n",
        "                    weights_file.write(\"Biases:\\n\")\n",
        "                    weights_file.write(str(weights[1]))\n",
        "                    weights_file.write(\"\\n\\n\")\n",
        "\n",
        "print(\"Weights and biases saved in 'WeightsAndBiases.txt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DtGA29r1fHV",
        "outputId": "3bf0c711-5b8f-4cc4-d218-0668e4983cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights and biases saved in 'WeightsAndBiases.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBY3sA60AAJS"
      },
      "outputs": [],
      "source": [
        "len(le_target.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpBnzIGfAHCr",
        "outputId": "9dac9214-7393-4f3d-d1ba-6e7003c27a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[['905', 'ec4', '18a', '4cc4', '67c', '4a03', '89f', '79f', 'a9a', '612', '17c3', '4e3e', 'b4a', 'f6', '515f', 'd5b', '6ca6', '376c', 'ae', '742d', '7fa', '7e1d', '7e57', 'ecb', 'dc6', '2107', 'fff', '5f5a', '2100', 'c52', 'f41', '1be', '3bc8', '0c3', '49', 'd04', '6f5', '7145', '0c2', '3ecb', 'f98', 'ca3', '557', '706d', 'e8f', '60b1', '85', '6155', '7e6', 'a9', '75', '186c', '507a', '127', '89e', '548a', '00c', '51fd', '7a88', '5e86', 'b9b', '2aaa', '4cfd', '50b', '1ec9', '709d', '161a', '5b96', '71b2', '377', '27c5', '7c0', '106', 'b3a', '626e', 'dbd', '2f5', '21c8', '1b8f', 'f7', '61a', 'e0a', '3e2', '216', '6931', 'fd7', '2', 'ed1', '23', 'd11', '618f', '52c7', '62af', 'a07', 'b19', '6f23', 'c30', '1a5', '0fa', '119', '27a', '4862', '7a7', 'bb4', '3ca2', '1bcb', '3ae', '629', '422f', '7a', 'b49', '631f', '23', '253a', '813', '0ff', '649', '6694', 'cea', '1d6', '1eda', '6ae5', '7a12', '526', '6fd', '7f1c', '11d6', '2d9'], ['588', 'adf', '6ac3', '702d', '706', '1a61', 'e8a', '3af5', '4897', '132', '1cf3', '7640', '816', 'ca1', 'cce', 'c7b', '9f1', '2cb9', '1d95', '8000', '93a', 'e38', '4cf9', 'cd3', '14c1', '55e9', '42a4', 'db', '7f1f', 'a29', '6bcd', '2f3', '2e1f', '4932', '5628', '8df', 'af3', '5954', '95d', 'dc', '069', '6f33', 'c37', '1ded', '649b', '203b', '4887', '50b8', '3fa6', 'fc2', 'b7d', '39c', '2bf', '74f2', '657e', '1657', '359f', '641d', '742e', '45bf', '6010', '410d', '9b', '16b'], ['53ab', '9b7', '15dd', '5247', 'b59', '3a1', 'b30', '22db', '3f', 'fff', 'a31', '891', 'd30', '9ac', 'ec9', '222d', '3dfe', '33c']]]\n",
            "Zero-point quantized Verilog parameters have been saved to 'nn_parameters_zero_point.vh'.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Predicted DDoS Attack Type: MSSQL\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def quantize_to_int16(weights, min_val=None, max_val=None):\n",
        "    \"\"\"\n",
        "    Quantizes weights to 16-bit signed integers and returns their hexadecimal representation.\n",
        "\n",
        "    Parameters:\n",
        "        weights (numpy array): The weights to quantize.\n",
        "        min_val (float): Minimum value for scaling. If None, the minimum of weights is used.\n",
        "        max_val (float): Maximum value for scaling. If None, the maximum of weights is used.\n",
        "\n",
        "    Returns:\n",
        "        hex_quantized_weights (list of str): List of hexadecimal strings representing quantized weights.\n",
        "        scale (float): Scaling factor used for quantization.\n",
        "        zero_point (int): Zero point for quantization.\n",
        "    \"\"\"\n",
        "    # Determine the range of weights\n",
        "    min_val = weights.min() if min_val is None else min_val\n",
        "    max_val = weights.max() if max_val is None else max_val\n",
        "\n",
        "    # Ensure the range is valid\n",
        "    if min_val == max_val:\n",
        "        raise ValueError(\"Min and max values must not be the same.\")\n",
        "\n",
        "    # Define the quantization range for int16\n",
        "    quant_min = -32768  # Minimum for int16\n",
        "    quant_max = 32767   # Maximum for int16\n",
        "\n",
        "    # Calculate scale and zero point\n",
        "    scale = (max_val - min_val) / (quant_max - quant_min)\n",
        "    zero_point = int(quant_min - min_val / scale)\n",
        "\n",
        "    # Quantize the weights\n",
        "    quantized_weights = (weights / scale + zero_point).round().astype(np.int16)\n",
        "\n",
        "    # Clip the quantized values to the 16-bit range to prevent overflow\n",
        "    quantized_weights = np.clip(quantized_weights, quant_min, quant_max)\n",
        "\n",
        "    # Convert quantized weights to hexadecimal representation\n",
        "    hex_quantized_weights = [f\"{str(hex(w))[3:]}\" for w in quantized_weights]\n",
        "\n",
        "    return hex_quantized_weights, scale, zero_point\n",
        "\n",
        "\n",
        "def to_hex(value):\n",
        "    \"\"\"Convert a signed 16-bit integer to hexadecimal.\"\"\"\n",
        "    return f\"{value & 0xFFFF:04X}\"\n",
        "\n",
        "\n",
        "# Zero-point quantization parameters\n",
        "scaling_factor = 0.1 # Chosen scale factor (can be adjusted based on the model's range)\n",
        "zero_point = 128  # Chosen zero point for asymmetric quantization (can be adjusted)\n",
        "\n",
        "# Assuming `model` is your trained neural network model\n",
        "weights = model.get_weights()[0::2]  # Extract weights (even indices)\n",
        "biases = model.get_weights()[1::2]  # Extract biases (odd indices)\n",
        "\n",
        "# Quantize weights and biases using zero-point quantization\n",
        "quantized_weights = [\n",
        "    [[quantize_to_int16(np.array(neuron_weights), scaling_factor)[0] for neuron_weights in layer_weights]]\n",
        "    for layer_weights in weights\n",
        "]\n",
        "\n",
        "quantized_biases = [\n",
        "    [quantize_to_int16(np.array(layer_biases), scaling_factor)[0] for layer_biases in biases]\n",
        "]\n",
        "\n",
        "# Generate Verilog parameters for weights and biases\n",
        "verilog_parameters = []\n",
        "\n",
        "# Weight parameters\n",
        "for layer_idx, layer_weights in enumerate(quantized_weights):\n",
        "    for neuron_idx, neuron_weights in enumerate(layer_weights):\n",
        "        for weight_idx, weight in enumerate(neuron_weights):\n",
        "            verilog_parameters.append(\n",
        "                f\"parameter [15:0] w{layer_idx+1}{neuron_idx+1}_{weight_idx+1} = 16'h{weight[0]};\"\n",
        "            )\n",
        "\n",
        "print(quantized_biases)\n",
        "# Bias parameters\n",
        "for layer_idx, layer_biases in enumerate(quantized_biases):\n",
        "    for neuron_idx, bias in enumerate(layer_biases):\n",
        "        verilog_parameters.append(\n",
        "            f\"parameter [15:0] b{layer_idx+1}_{neuron_idx+1} = 16'h{bias[0]};\"\n",
        "        )\n",
        "\n",
        "# Save Verilog parameters to a file\n",
        "with open(\"nn_parameters_zero_point.v\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(verilog_parameters))\n",
        "\n",
        "print(\"Zero-point quantized Verilog parameters have been saved to 'nn_parameters_zero_point.vh'.\")\n",
        "\n",
        "# Example usage: Predict the attack type (assuming model and data are ready)\n",
        "sample_prediction = model.predict(X_test[:1])  # Predict for the first sample in the test set\n",
        "predicted_class = le_target.inverse_transform([np.argmax(sample_prediction)])\n",
        "print(f\"Predicted DDoS Attack Type: {predicted_class[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orMxb8Z7u1u4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DMAsw1_BjyV",
        "outputId": "dec40428-a799-49f4-b9df-da2c116ab6c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Benign', 'DrDoS_DNS', 'DrDoS_LDAP', 'DrDoS_MSSQL', 'DrDoS_NTP',\n",
              "       'DrDoS_NetBIOS', 'DrDoS_SNMP', 'DrDoS_UDP', 'LDAP', 'MSSQL',\n",
              "       'NetBIOS', 'Portmap', 'Syn', 'TFTP', 'UDP', 'UDP-lag', 'UDPLag',\n",
              "       'WebDDoS'], dtype=object)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "le_target.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEtWozYSBekF",
        "outputId": "ddd539e3-bbac-4723-eec2-9746968aa203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[4.19882526e-05 2.43530981e-02 4.04087950e-05 3.03742707e-01\n",
            "  1.03227526e-01 6.38544699e-03 3.06867034e-04 1.66013651e-02\n",
            "  2.65607378e-04 5.02128839e-01 1.17157042e-05 4.72763146e-04\n",
            "  1.10186310e-03 1.23460370e-03 3.74713652e-02 2.12648069e-03\n",
            "  4.85996483e-04 1.37321570e-06]]\n"
          ]
        }
      ],
      "source": [
        "print(sample_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "fMfHTKOZATN7",
        "outputId": "b6d59836-6abe-4613-e342-45ae06c9cd4f"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-61805143ce78>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Example Usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mquantized_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantized_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_quantized_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m# Replace this with your actual input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0minput_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-f8a736af1fa3>\u001b[0m in \u001b[0;36mload_quantized_parameters\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mint_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhex_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                         \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "# prompt: using above weights predict output from quantized values\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load the quantized weights and bias\n",
        "\n",
        "def predict_with_quantized_weights(input_data, weights, biases):\n",
        "    \"\"\"Simulate forward pass using quantized weights and biases.\"\"\"\n",
        "\n",
        "    # Reshape input data if necessary\n",
        "    input_data = np.array(input_data).reshape(1, -1)\n",
        "\n",
        "    activations = [input_data]\n",
        "    for layer in range(1, len(weights) + 1):\n",
        "        layer_output = []\n",
        "        for neuron in range(1, len(weights[layer]) + 1):\n",
        "            weighted_sum = biases[layer][neuron] / scaling_factor\n",
        "            for weight_idx, weight in weights[layer][neuron].items():\n",
        "                 weighted_sum += (weight / scaling_factor) * activations[-1][0][weight_idx-1]\n",
        "\n",
        "            # Apply ReLU activation for hidden layers\n",
        "            if layer < len(weights):\n",
        "                output = max(0, weighted_sum)\n",
        "            else:  # Softmax for output layer (simplified for demonstration)\n",
        "                output = weighted_sum\n",
        "            layer_output.append(output)\n",
        "        activations.append(np.array([layer_output]))\n",
        "\n",
        "    return activations[-1]\n",
        "\n",
        "\n",
        "# Example Usage:\n",
        "quantized_weights, quantized_biases = load_quantized_parameters()\n",
        "# Replace this with your actual input data\n",
        "input_sample = X_test[:1].tolist()[0]\n",
        "\n",
        "quantized_prediction = predict_with_quantized_weights(input_sample, quantized_weights, quantized_biases)\n",
        "print(\"Predicted Output (Quantized):\", np.argmax(quantized_prediction))\n",
        "# ... (Rest of your code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "ULZmy3U8DcZr",
        "outputId": "3321cf47-e574-40aa-d932-2347ab65a827"
      },
      "outputs": [
        {
          "ename": "UnboundLocalError",
          "evalue": "local variable 'neuron_idx' referenced before assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-51f688092501>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the weights and biases from the Verilog file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mquantized_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantized_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_quantized_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nn_parameters_zero_point.vh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define the scale and zero-point used during quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m  \u001b[0;31m# Adjust this based on your quantization settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-1f4f3540e704>\u001b[0m in \u001b[0;36mload_quantized_parameters\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlayer_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mneuron_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                         \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneuron_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneuron_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'neuron_idx' referenced before assignment"
          ]
        }
      ],
      "source": [
        "# Load the weights and biases from the Verilog file\n",
        "quantized_weights, quantized_biases = load_quantized_parameters(\"nn_parameters_zero_point.vh\")\n",
        "\n",
        "# Define the scale and zero-point used during quantization\n",
        "scale = 0.05  # Adjust this based on your quantization settings\n",
        "zero_point = 128  # Adjust this based on your quantization settings\n",
        "\n",
        "# Example input data (replace with actual test input)\n",
        "input_sample = np.random.uniform(-1, 1, size=10).tolist()  # Adjust size to match input dimensions\n",
        "\n",
        "# Predict using the quantized parameters\n",
        "quantized_prediction = predict_with_zero_point_quantized_weights(\n",
        "    input_sample, quantized_weights, quantized_biases, scale, zero_point\n",
        ")\n",
        "\n",
        "# Print the raw output and predicted class\n",
        "print(\"Predicted Output (Quantized):\", quantized_prediction)\n",
        "predicted_class = np.argmax(quantized_prediction)\n",
        "print(\"Predicted Class:\", predicted_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzNybWw4BJBR"
      },
      "outputs": [],
      "source": [
        "# prompt: ValueError                                Traceback (most recent call last)\n",
        "# <ipython-input-33-51f688092501> in <cell line: 2>()\n",
        "#       1 # Load the weights and biases from the Verilog file\n",
        "# ----> 2 quantized_weights, quantized_biases = load_quantized_parameters(\"nn_parameters_zero_point.vh\")\n",
        "#       3\n",
        "#       4 # Define the scale and zero-point used during quantization\n",
        "#       5 scale = 0.05  # Adjust this based on your quantization settings\n",
        "# <ipython-input-32-0d9c3dc8a40d> in load_quantized_parameters(filename)\n",
        "#      30                     hex_value = hex_value.replace(\";\", \"\")\n",
        "#      31\n",
        "# ---> 32                     int_value = int(hex_value, 16) # Changed from base 8 to base 16\n",
        "#      33\n",
        "#      34                     if param_name.startswith(\"w\"):\n",
        "# ValueError: invalid literal for int() with base 16: \"8'h85\"\n",
        "\n",
        "import re\n",
        "\n",
        "def load_quantized_parameters(filename):\n",
        "    \"\"\"Loads quantized weights and biases from a Verilog file.\"\"\"\n",
        "    weights = {}\n",
        "    biases = {}\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            match = re.match(r\"parameter \\[7:0] (\\w+)(\\d+)_?(\\d+)? = 8'h([0-9a-fA-F]+);\", line)\n",
        "            if match:\n",
        "                param_name = match.group(1)\n",
        "                layer_idx = int(match.group(2))\n",
        "                if match.group(3):\n",
        "                  neuron_idx = int(match.group(3))\n",
        "                hex_value = match.group(4)\n",
        "                int_value = int(hex_value, 16)\n",
        "\n",
        "                if param_name.startswith(\"w\"):\n",
        "                    if layer_idx not in weights:\n",
        "                        weights[layer_idx] = {}\n",
        "                    if neuron_idx not in weights[layer_idx]:\n",
        "                        weights[layer_idx][neuron_idx] = {}\n",
        "                    weights[layer_idx][neuron_idx][int(match.group(3))] = int_value\n",
        "                elif param_name.startswith(\"b\"):\n",
        "                    if layer_idx not in biases:\n",
        "                        biases[layer_idx] = {}\n",
        "                    biases[layer_idx][neuron_idx] = int_value\n",
        "    return weights, biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BBQcWCgZFmXS",
        "outputId": "6844006d-7074-4b5d-8f97-0ef7da28db6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting hls4ml\n",
            "  Downloading hls4ml-1.0.0-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting calmjs.parse (from hls4ml)\n",
            "  Downloading calmjs.parse-1.3.2-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from hls4ml) (3.12.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hls4ml) (1.26.4)\n",
            "Collecting onnx>=1.4.0 (from hls4ml)\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting pydigitalwavetools==1.1 (from hls4ml)\n",
            "  Downloading pyDigitalWaveTools-1.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from hls4ml) (3.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from hls4ml) (6.0.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from hls4ml) (0.9.0)\n",
            "Collecting tensorflow<=2.14.1,>=2.8.0 (from hls4ml)\n",
            "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow-model-optimization<=0.7.5 (from hls4ml)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.4.0->hls4ml) (4.25.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (18.1.1)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow<=2.14.1,>=2.8.0->hls4ml)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow<=2.14.1,>=2.8.0->hls4ml)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.14.1,>=2.8.0->hls4ml) (1.68.1)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow<=2.14.1,>=2.8.0->hls4ml)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow<=2.14.1,>=2.8.0->hls4ml)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow<=2.14.1,>=2.8.0->hls4ml)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization<=0.7.5->hls4ml) (0.1.8)\n",
            "Requirement already satisfied: ply>=3.6 in /usr/local/lib/python3.10/dist-packages (from calmjs.parse->hls4ml) (3.11)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<=2.14.1,>=2.8.0->hls4ml) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<=2.14.1,>=2.8.0->hls4ml) (3.2.2)\n",
            "Downloading hls4ml-1.0.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyDigitalWaveTools-1.1-py3-none-any.whl (13 kB)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading calmjs.parse-1.3.2-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: pydigitalwavetools, wrapt, tensorflow-model-optimization, tensorflow-estimator, onnx, ml-dtypes, keras, calmjs.parse, google-auth-oauthlib, tensorboard, tensorflow, hls4ml\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.0\n",
            "    Uninstalling wrapt-1.17.0:\n",
            "      Successfully uninstalled wrapt-1.17.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.69 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed calmjs.parse-1.3.2 google-auth-oauthlib-1.0.0 hls4ml-1.0.0 keras-2.14.0 ml-dtypes-0.2.0 onnx-1.17.0 pydigitalwavetools-1.1 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tensorflow-model-optimization-0.7.5 wrapt-1.14.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "45b07054dbfa42c98a1613a7f48f69e8",
              "pip_warning": {
                "packages": [
                  "google_auth_oauthlib",
                  "keras",
                  "ml_dtypes",
                  "tensorflow",
                  "wrapt"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install hls4ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "nrUo7MlQFklb",
        "outputId": "1f018061-c625-4fd1-808f-98f674ee2f17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QKeras installation not found, installing one...\n",
            "QKeras installed.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.keras.utils'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-7beea6ac64f6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# End of workaround\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa: F401, E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/converters/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_to_hls\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasFileReader\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_to_hls\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasModelReader\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_to_hls\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasReader\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/converters/keras_to_hls.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mMAXMULT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/model/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHLSConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelGraph\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprofiling\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/model/graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypeslib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayer_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/backends/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_available_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpga\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpga_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFPGABackend\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moneapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moneapi_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneAPIBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquartus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquartus_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuartusBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymbolicExpressionBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/backends/backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_backend_flows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m from hls4ml.model.optimizer import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/backends/template.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizerPass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOptimizerPass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"The Template base class, should not be instantiated directly\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/model/optimizer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from hls4ml.model.optimizer.optimizer import (  # noqa: F401\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mConfigurableOptimizerPass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mGlobalOptimizerPass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/model/optimizer/optimizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_to_snake_case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_from_keras_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_from_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_from_pytorch_model\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_example_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_example_model\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hls4ml/utils/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mqkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/qkeras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mb2t\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqconv2d_batchnorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQConv2DBatchnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/qkeras/b2t.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\"\"\"Implements total/partial Binary to Thermometer decoder.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import hls4ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "iArhoFuFBQkw",
        "outputId": "80241464-d69e-4c8b-f7e3-0b5854699d1a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'hls4ml' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-6ea67f1bef6e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert Keras model to HLS model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhls_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhls4ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_from_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhls_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hls4ml_config.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compile the HLS model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhls_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hls4ml' is not defined"
          ]
        }
      ],
      "source": [
        "# Convert Keras model to HLS model\n",
        "hls_model = hls4ml.convert_from_keras(model, hls_config='hls4ml_config.yaml')\n",
        "\n",
        "# Compile the HLS model\n",
        "hls_model.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1gujj5uBFjXj",
        "outputId": "d1468c50-615a-46a2-d4cc-ac084dc0a8f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 2.8951\n",
            "Epoch [2/10], Loss: 2.8618\n",
            "Epoch [3/10], Loss: 2.8282\n",
            "Epoch [4/10], Loss: 2.7942\n",
            "Epoch [5/10], Loss: 2.7604\n",
            "Epoch [6/10], Loss: 2.7261\n",
            "Epoch [7/10], Loss: 2.6908\n",
            "Epoch [8/10], Loss: 2.6547\n",
            "Epoch [9/10], Loss: 2.6171\n",
            "Epoch [10/10], Loss: 2.5781\n",
            "Test Accuracy: 55.04%\n",
            "Model saved!\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, model: nn.Sequential):\n",
        "        self.model = model\n",
        "        self.layers = []\n",
        "\n",
        "    def __str__(self):\n",
        "        return '\\n'.join(str(layer) for layer in self.layers)\n",
        "\n",
        "    def parse_layers(self):\n",
        "        for i, layer in enumerate(self.model):\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                self.layers.append(layer.state_dict()['weight'])\n",
        "                self.layers.append(layer.state_dict()['bias'])\n",
        "            elif isinstance(layer, nn.ReLU):\n",
        "                self.layers.append(nn.ReLU(self.model[i - 1].out_features, i))\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown layer type {layer}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.model(x)\n",
        "\n",
        "    def forward_range(self, ranges: List[List[float]]):\n",
        "        start = np.array(ranges)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            start = layer.forward_range(start)\n",
        "\n",
        "    def get_vars(self):\n",
        "        in_params = [f\"in{i}\" for i in range(self.layers[0].shape[0])]\n",
        "        out_params = [f\"out{i}\" for i in range(self.layers[-1].shape[-1])]\n",
        "\n",
        "        in_definitions = [f\"    input [{self.layers[0].in_bits[i] - 1}:0] {in_params[i]};\"\n",
        "                          for i in range(self.layers[0].shape[0])]\n",
        "\n",
        "        out_definitions = [f\"    output [{self.layers[-1].out_bits[i] - 1}:0] {out_params[i]};\"\n",
        "                           for i in range(self.layers[-1].shape[-1])]\n",
        "\n",
        "        return in_params, out_params, in_definitions, out_definitions\n",
        "\n",
        "    def emit(self):\n",
        "        out = [\"`timescale 1ns / 1ps\"]\n",
        "\n",
        "        in_params, out_params, in_definitions, out_definitions = self.get_vars()\n",
        "\n",
        "        top = [\n",
        "            f\"module top({','.join(in_params)}, {','.join(out_params)});\",\n",
        "            *in_definitions,\n",
        "            *out_definitions,\n",
        "        ]\n",
        "\n",
        "        in_wires = in_params\n",
        "        out_wires = []\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            out.append(layer.emit())\n",
        "\n",
        "            out_wires = []\n",
        "            for j in range(layer.shape[-1]):\n",
        "                top.append(f\"    wire [{layer.out_bits[j]}:0] layer_{i}_out_{j};\")\n",
        "                out_wires.append(f\"layer_{i}_out_{j}\")\n",
        "\n",
        "            top.append(f\"    {layer.name} layer_{i}({','.join(in_wires)}, {','.join(out_wires)});\")\n",
        "\n",
        "            in_wires = out_wires\n",
        "\n",
        "        assigns = [f\"    assign out{i} = {out_wire};\" for i, out_wire in enumerate(out_wires)]\n",
        "\n",
        "        top.extend(assigns)\n",
        "        top.append(\"endmodule\")\n",
        "\n",
        "        out.append('\\n'.join(top))\n",
        "\n",
        "        return '\\n'.join(out)\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    # Load the dataset\n",
        "    data = pd.read_parquet(\"/content/data.parquet\")\n",
        "\n",
        "    # Feature selection and preprocessing\n",
        "    selected_features = [\n",
        "        \"Protocol\", \"Flow Duration\", \"Total Fwd Packets\", \"Total Backward Packets\",\n",
        "        \"Fwd Packets Length Total\", \"Bwd Packets Length Total\", \"Fwd Packet Length Max\",\n",
        "        \"Fwd Packet Length Min\", \"Fwd Packet Length Mean\", \"Fwd Packet Length Std\"\n",
        "    ]\n",
        "    target_column = \"Label\"\n",
        "\n",
        "    le_target = LabelEncoder()\n",
        "    data[target_column] = le_target.fit_transform(data[target_column])\n",
        "\n",
        "    X = data[selected_features]\n",
        "    y = data[target_column]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Define the PyTorch model\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(len(selected_features), 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(64, len(le_target.classes_)),\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    num_epochs = 10\n",
        "    batch_size = 32\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(X_train_tensor)\n",
        "        loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test_tensor)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        accuracy = (predictions == y_test_tensor).float().mean()\n",
        "        print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"ddos_attack_type_model.pt\")\n",
        "    print(\"Model saved!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "yAVaDR80i2q7",
        "outputId": "5f1ffa36-f984-45ac-d7f1-a4ce17c5b5c5"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Linear' object has no attribute 'Linear'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-887e72c84f55>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Parse the layers to prepare for Verilog generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Generate Verilog code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d2da3dd982e8>\u001b[0m in \u001b[0;36mparse_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'Linear'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "# import layers\n",
        "# Define a simple PyTorch model\n",
        "simple_model = nn.Sequential(\n",
        "        nn.Linear(9, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(64, 18),\n",
        "    )\n",
        "\n",
        "# Wrap the model in the custom Model class\n",
        "model = Model(simple_model)\n",
        "\n",
        "# Parse the layers to prepare for Verilog generation\n",
        "model.parse_layers()\n",
        "\n",
        "# Generate Verilog code\n",
        "verilog_code = model.emit()\n",
        "\n",
        "# Save the Verilog code to a file\n",
        "with open(\"generated_model.v\", \"w\") as f:\n",
        "    f.write(verilog_code)\n",
        "\n",
        "print(\"Verilog code has been generated and saved as 'generated_model.v'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "HWXYmUwdjF_-",
        "outputId": "5f4e1bdf-32d7-4088-f1b1-b7737167382c"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data.parquet'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4ea719928443>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtarget_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Label'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/data.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m# Check the data structure and clean if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data shape after concatenation:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_blocks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mfilesystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data.parquet'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "# Selected features based on the data you shared\n",
        "selected_features = [\n",
        "    'Protocol',\n",
        "    'Flow Duration',\n",
        "    'Total Fwd Packets',\n",
        "    'Total Backward Packets',\n",
        "    'Fwd Packets Length Total',\n",
        "    'Bwd Packets Length Total',\n",
        "    'Fwd Packet Length Max',\n",
        "    'Fwd Packet Length Min',\n",
        "    'Fwd Packet Length Mean',\n",
        "    'Fwd Packet Length Std'\n",
        "]\n",
        "\n",
        "target_column = 'Label'\n",
        "\n",
        "data = pd.read_parquet(\"/content/data.parquet\")\n",
        "# Check the data structure and clean if necessary\n",
        "print(\"Data shape after concatenation:\", data.shape)\n",
        "\n",
        "# Encode the target variable\n",
        "le_target = LabelEncoder()\n",
        "data[target_column] = le_target.fit_transform(data[target_column])\n",
        "\n",
        "# Split features and target\n",
        "X = data[selected_features]\n",
        "y = data[target_column]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "nn = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = nn.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "weights = nn.coefs_\n",
        "biases = nn.intercepts_\n",
        "\n",
        "def quantize_to_fixed_point(value, scaling_factor=2048):\n",
        "    \"\"\"Quantize a float value to a fixed-point 16-bit integer and clamp to the 16-bit range.\"\"\"\n",
        "    fixed_point_value = int(value * scaling_factor)\n",
        "    if fixed_point_value < -32768:\n",
        "        fixed_point_value = -32768\n",
        "    elif fixed_point_value > 32767:\n",
        "        fixed_point_value = 32767\n",
        "    return fixed_point_value\n",
        "\n",
        "def to_hex(value):\n",
        "    \"\"\"Convert a signed 16-bit integer to hexadecimal.\"\"\"\n",
        "    return f\"{value & 0xFFFF:04X}\"\n",
        "\n",
        "\n",
        "quantized_weights = [\n",
        "    [\n",
        "        [quantize_to_fixed_point(weight, scaling_factor=2048) for weight in neuron_weights]\n",
        "        for neuron_weights in layer_weights\n",
        "    ]\n",
        "    for layer_weights in weights\n",
        "]\n",
        "\n",
        "quantized_biases = [\n",
        "    [quantize_to_fixed_point(bias, scaling_factor=2048) for bias in layer_biases]\n",
        "    for layer_biases in biases\n",
        "]\n",
        "\n",
        "verilog_parameters = []\n",
        "\n",
        "for layer_idx, layer_weights in enumerate(quantized_weights):\n",
        "    for neuron_idx, neuron_weights in enumerate(layer_weights):\n",
        "        for weight_idx, weight in enumerate(neuron_weights):\n",
        "            verilog_parameters.append(\n",
        "                f\"parameter signed [15:0] w{layer_idx+1}{neuron_idx+1}_{weight_idx+1} = 16'h{to_hex(weight)};\"\n",
        "            )\n",
        "\n",
        "\n",
        "for layer_idx, layer_biases in enumerate(quantized_biases):\n",
        "    for neuron_idx, bias in enumerate(layer_biases):\n",
        "        verilog_parameters.append(\n",
        "            f\"parameter signed [15:0] b{layer_idx+1}_{neuron_idx+1} = 16'h{to_hex(bias)};\"\n",
        "        )\n",
        "\n",
        "\n",
        "print(\"\\n\".join(verilog_parameters))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "haHd0LIgDKHl",
        "outputId": "2c0710a0-85e3-4603-a9f2-8a381623419d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'accuracy' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0d460b0c6745>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
          ]
        }
      ],
      "source": [
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SPIixoP4wHy",
        "outputId": "dab22fb0-2de0-4aa1-8bd2-0c252fdc67ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row 1: Protocol = 6, Hexadecimal = 0000\n",
            "Row 2: Protocol = 6, Hexadecimal = 0000\n",
            "Row 3: Protocol = 6, Hexadecimal = 0000\n",
            "Row 4: Protocol = 6, Hexadecimal = 0000\n",
            "Row 5: Protocol = 6, Hexadecimal = 0000\n",
            "Row 1: Flow Duration = 105, Hexadecimal = 0000\n",
            "Row 2: Flow Duration = 1, Hexadecimal = 0000\n",
            "Row 3: Flow Duration = 55295858, Hexadecimal = 0800\n",
            "Row 4: Flow Duration = 49, Hexadecimal = 0000\n",
            "Row 5: Flow Duration = 109, Hexadecimal = 0000\n",
            "Row 1: Total Fwd Packets = 2, Hexadecimal = 0000\n",
            "Row 2: Total Fwd Packets = 2, Hexadecimal = 0000\n",
            "Row 3: Total Fwd Packets = 8, Hexadecimal = 0800\n",
            "Row 4: Total Fwd Packets = 2, Hexadecimal = 0000\n",
            "Row 5: Total Fwd Packets = 2, Hexadecimal = 0000\n",
            "Row 1: Total Backward Packets = 2, Hexadecimal = 0800\n",
            "Row 2: Total Backward Packets = 0, Hexadecimal = 0000\n",
            "Row 3: Total Backward Packets = 0, Hexadecimal = 0000\n",
            "Row 4: Total Backward Packets = 0, Hexadecimal = 0000\n",
            "Row 5: Total Backward Packets = 2, Hexadecimal = 0800\n",
            "Row 1: Fwd Packets Length Total = 12.0, Hexadecimal = 0000\n",
            "Row 2: Fwd Packets Length Total = 12.0, Hexadecimal = 0000\n",
            "Row 3: Fwd Packets Length Total = 48.0, Hexadecimal = 0800\n",
            "Row 4: Fwd Packets Length Total = 12.0, Hexadecimal = 0000\n",
            "Row 5: Fwd Packets Length Total = 12.0, Hexadecimal = 0000\n",
            "Row 1: Bwd Packets Length Total = 12.0, Hexadecimal = 0800\n",
            "Row 2: Bwd Packets Length Total = 0.0, Hexadecimal = 0000\n",
            "Row 3: Bwd Packets Length Total = 0.0, Hexadecimal = 0000\n",
            "Row 4: Bwd Packets Length Total = 0.0, Hexadecimal = 0000\n",
            "Row 5: Bwd Packets Length Total = 12.0, Hexadecimal = 0800\n",
            "Row 1: Fwd Packet Length Max = 6.0, Hexadecimal = 0000\n",
            "Row 2: Fwd Packet Length Max = 6.0, Hexadecimal = 0000\n",
            "Row 3: Fwd Packet Length Max = 6.0, Hexadecimal = 0000\n",
            "Row 4: Fwd Packet Length Max = 6.0, Hexadecimal = 0000\n",
            "Row 5: Fwd Packet Length Max = 6.0, Hexadecimal = 0000\n",
            "Row 1: Fwd Packet Length Min = 6.0, Hexadecimal = 0000\n",
            "Row 2: Fwd Packet Length Min = 6.0, Hexadecimal = 0000\n",
            "Row 3: Fwd Packet Length Min = 6.0, Hexadecimal = 0000\n",
            "Row 4: Fwd Packet Length Min = 6.0, Hexadecimal = 0000\n",
            "Row 5: Fwd Packet Length Min = 6.0, Hexadecimal = 0000\n",
            "Row 1: Fwd Packet Length Mean = 6.0, Hexadecimal = 0000\n",
            "Row 2: Fwd Packet Length Mean = 6.0, Hexadecimal = 0000\n",
            "Row 3: Fwd Packet Length Mean = 6.0, Hexadecimal = 0000\n",
            "Row 4: Fwd Packet Length Mean = 6.0, Hexadecimal = 0000\n",
            "Row 5: Fwd Packet Length Mean = 6.0, Hexadecimal = 0000\n",
            "Row 1: Fwd Packet Length Std = 0.0, Hexadecimal = 0000\n",
            "Row 2: Fwd Packet Length Std = 0.0, Hexadecimal = 0000\n",
            "Row 3: Fwd Packet Length Std = 0.0, Hexadecimal = 0000\n",
            "Row 4: Fwd Packet Length Std = 0.0, Hexadecimal = 0000\n",
            "Row 5: Fwd Packet Length Std = 0.0, Hexadecimal = 0000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def quantize_and_convert_to_hex(value, min_val, max_val):\n",
        "    \"\"\"Quantize the value and convert it to 16-bit signed hexadecimal.\"\"\"\n",
        "    # Avoid division by zero if min_val == max_val\n",
        "    if min_val == max_val:\n",
        "        # If all values are the same, return a fixed hexadecimal value\n",
        "        quantized = 0  # or you could set this to any valid value within the range\n",
        "    else:\n",
        "        # Normalize to the range [0, 1]\n",
        "        quantized = int(2048 * (value - min_val) / (max_val - min_val))\n",
        "\n",
        "    # Clamp to 16-bit signed range\n",
        "    if quantized < -32768:\n",
        "        quantized = -32768\n",
        "    elif quantized > 32767:\n",
        "        quantized = 32767\n",
        "\n",
        "    # Convert to hexadecimal\n",
        "    return f\"{quantized & 0xFFFF:04X}\"\n",
        "\n",
        "# Load the parquet file\n",
        "data = pd.read_parquet('/content/data.parquet')\n",
        "\n",
        "# Select the first 5 rows\n",
        "data = data.head(5)\n",
        "\n",
        "# Define columns to process\n",
        "columns_to_process = [\n",
        "    'Protocol',\n",
        "    'Flow Duration',\n",
        "    'Total Fwd Packets',\n",
        "    'Total Backward Packets',\n",
        "    'Fwd Packets Length Total',\n",
        "    'Bwd Packets Length Total',\n",
        "    'Fwd Packet Length Max',\n",
        "    'Fwd Packet Length Min',\n",
        "    'Fwd Packet Length Mean',\n",
        "    'Fwd Packet Length Std'\n",
        "]\n",
        "\n",
        "# Iterate through each column\n",
        "for col in columns_to_process:\n",
        "    # Normalizing the column\n",
        "    values = data[col]\n",
        "    min_value = min(values)\n",
        "    max_value = max(values)\n",
        "\n",
        "    # Generate hex values for the column\n",
        "    hex_values = [quantize_and_convert_to_hex(val, min_value, max_value) for val in values]\n",
        "\n",
        "    # Print results for the first 5 rows\n",
        "    for i, hex_val in enumerate(hex_values):\n",
        "        print(f\"Row {i + 1}: {col} = {data[col].iloc[i]}, Hexadecimal = {hex_val}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GubJhLi7bv3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# module layer9x18 (\n",
        "#     input signed [15:0] in1,\n",
        "#     input signed [15:0] in2,\n",
        "#     input signed [15:0] in3,\n",
        "#     input signed [15:0] in4,\n",
        "#     input signed [15:0] in5,\n",
        "#     input signed [15:0] in6,\n",
        "#     input signed [15:0] in7,\n",
        "#     input signed [15:0] in8,\n",
        "#     input signed [15:0] in9,\n",
        "#     output signed [15:0] out1,\n",
        "#     output signed [15:0] out2,\n",
        "#     output signed [15:0] out3,\n",
        "#     output signed [15:0] out4,\n",
        "#     output signed [15:0] out5,\n",
        "#     output signed [15:0] out6,\n",
        "#     output signed [15:0] out7,\n",
        "#     output signed [15:0] out8,\n",
        "#     output signed [15:0] out9,\n",
        "#     output signed [15:0] out10,\n",
        "#     output signed [15:0] out11,\n",
        "#     output signed [15:0] out12,\n",
        "#     output signed [15:0] out13,\n",
        "#     output signed [15:0] out14,\n",
        "#     output signed [15:0] out15,\n",
        "#     output signed [15:0] out16,\n",
        "#     output signed [15:0] out17,\n",
        "#     output signed [15:0] out18\n",
        "# );\n",
        "\n",
        "# // Declare parameters (weights and biases)\n",
        "# parameter signed [15:0] w11_1 = 16'h046C, w11_2 = 16'h19A5;\n",
        "# parameter signed [15:0] w12_1 = 16'hF5A9, w12_2 = 16'hEA35;\n",
        "# parameter signed [15:0] w13_1 = 16'h048C, w13_2 = 16'h281E;\n",
        "# parameter signed [15:0] w14_1 = 16'h0B22, w14_2 = 16'h15AE;\n",
        "# parameter signed [15:0] w15_1 = 16'h3EF8, w15_2 = 16'h38EB;\n",
        "# parameter signed [15:0] w16_1 = 16'hFFE2, w16_2 = 16'h0F95;\n",
        "# parameter signed [15:0] w17_1 = 16'hD939, w17_2 = 16'h0850;\n",
        "# parameter signed [15:0] w18_1 = 16'hC026, w18_2 = 16'hD273;\n",
        "# parameter signed [15:0] w19_1 = 16'hB73F, w19_2 = 16'h0491;\n",
        "# parameter signed [15:0] w110_1 = 16'h2420, w110_2 = 16'h3396;\n",
        "# parameter signed [15:0] w21_1 = 16'h1A02, w21_2 = 16'hFF0D, w21_3 = 16'hE001;\n",
        "# parameter signed [15:0] w21_4 = 16'hD801, w21_5 = 16'hF4CD, w21_6 = 16'h05A6;\n",
        "# parameter signed [15:0] w21_7 = 16'hEAC8, w21_8 = 16'h024B, w21_9 = 16'hDCB5;\n",
        "# parameter signed [15:0] w21_10 = 16'hD8AD, w21_11 = 16'h089A, w21_12 = 16'h09D9;\n",
        "# parameter signed [15:0] w21_13 = 16'h1D6D, w21_14 = 16'h9D4B, w21_15 = 16'h0158;\n",
        "# parameter signed [15:0] w21_16 = 16'h0FD9, w21_17 = 16'hFBDF, w21_18 = 16'h033F;\n",
        "# parameter signed [15:0] w22_1 = 16'hF48D, w22_2 = 16'hF261, w22_3 = 16'hE927;\n",
        "# parameter signed [15:0] w22_4 = 16'h05F7, w22_5 = 16'h1015, w22_6 = 16'hF9C9;\n",
        "# parameter signed [15:0] w22_7 = 16'hEDC9, w22_8 = 16'h01AC, w22_9 = 16'hEC9D;\n",
        "# parameter signed [15:0] w22_10 = 16'h0590, w22_11 = 16'hFAA5, w22_12 = 16'hF73F;\n",
        "# parameter signed [15:0] w22_13 = 16'hDD2D, w22_14 = 16'h1629, w22_15 = 16'h0414;\n",
        "# parameter signed [15:0] w22_16 = 16'hEFD7, w22_17 = 16'hF4B6, w22_18 = 16'hF07B;\n",
        "\n",
        "# parameter signed [15:0] b1_1 = 16'h185D, b1_2 = 16'hFF97;\n",
        "# parameter signed [15:0] b2_1 = 16'hF424, b2_2 = 16'h121C, b2_3 = 16'h10E4;\n",
        "# parameter signed [15:0] b2_4 = 16'h0742, b2_5 = 16'hF9B5, b2_6 = 16'hF24A;\n",
        "# parameter signed [15:0] b2_7 = 16'h1599, b2_8 = 16'hFA24, b2_9 = 16'h130E;\n",
        "# parameter signed [15:0] b2_10 = 16'h0A80, b2_11 = 16'hED23, b2_12 = 16'hF359;\n",
        "# parameter signed [15:0] b2_13 = 16'hF78A, b2_14 = 16'h070D, b2_15 = 16'hF9E8;\n",
        "# parameter signed [15:0] b2_16 = 16'h04AF, b2_17 = 16'hF1D7, b2_18 = 16'hF234;\n",
        "\n",
        "# // Implement weighted sums with bias for each output\n",
        "# assign out1  = (in1 * w11_1 + in2 * w11_2 + in3 * w12_1 + in4 * w12_2 + in5 * w13_1 + in6 * w13_2 + in7 * w14_1 + in8 * w14_2 + in9 * w15_1) >>> 8 + b1_1;\n",
        "# assign out2  = (in1 * w16_1 + in2 * w16_2 + in3 * w17_1 + in4 * w17_2 + in5 * w18_1 + in6 * w18_2 + in7 * w19_1 + in8 * w19_2 + in9 * w110_1) >>> 8 + b1_2;\n",
        "# assign out3  = (in1 * w110_2 + in2 * w21_1 + in3 * w21_2 + in4 * w21_3 + in5 * w21_4 + in6 * w21_5 + in7 * w21_6 + in8 * w21_7 + in9 * w21_8) >>> 8 + b2_1;\n",
        "# assign out4  = (in1 * w21_9 + in2 * w21_10 + in3 * w21_11 + in4 * w21_12 + in5 * w21_13 + in6 * w21_14 + in7 * w21_15 + in8 * w21_16 + in9 * w21_17) >>> 8 + b2_2;\n",
        "# assign out5  = (in1 * w21_18 + in2 * w22_1 + in3 * w22_2 + in4 * w22_3 + in5 * w22_4 + in6 * w22_5 + in7 * w22_6 + in8 * w22_7 + in9 * w22_8) >>> 8 + b2_3;\n",
        "# assign out6  = (in1 * w22_9 + in2 * w22_10 + in3 * w22_11 + in4 * w22_12 + in5 * w22_13 + in6 * w22_14 + in7 * w22_15 + in8 * w22_16 + in9 * w22_17) >>> 8 + b2_4;\n",
        "# assign out7  = (in1 * w22_18 + in2 * w11_1 + in3 * w11_2 + in4 * w12_1 + in5 * w12_2 + in6 * w13_1 + in7 * w13_2 + in8 * w14_1 + in9 * w14_2) >>> 8 + b2_5;\n",
        "# assign out8  = (in1 * w15_1 + in2 * w15_2 + in3 * w16_1 + in4 * w16_2 + in5 * w17_1 + in6 * w17_2 + in7 * w18_1 + in8 * w18_2 + in9 * w19_1) >>> 8 + b2_6;\n",
        "# assign out9  = (in1 * w19_2 + in2 * w110_1 + in3 * w110_2 + in4 * w21_1 + in5 * w21_2 + in6 * w21_3 + in7 * w21_4 + in8 * w21_5 + in9 * w21_6) >>> 8 + b2_7;\n",
        "# assign out10 = (in1 * w21_7 + in2 * w21_8 + in3 * w21_9 + in4 * w21_10 + in5 * w21_11 + in6 * w21_12 + in7 * w21_13 + in8 * w21_14 + in9 * w21_15) >>> 8 + b2_8;\n",
        "# assign out11 = (in1 * w21_16 + in2 * w21_17 + in3 * w21_18 + in4 * w22_1 + in5 * w22_2 + in6 * w22_3 + in7 * w22_4 + in8 * w22_5 + in9 * w22_6) >>> 8 + b2_9;\n",
        "# assign out12 = (in1 * w22_7 + in2 * w22_8 + in3 * w22_9 + in4 * w22_10 + in5 * w22_11 + in6 * w22_12 + in7 * w22_13 + in8 * w22_14 + in9 * w22_15) >>> 8 + b2_10;\n",
        "# assign out13 = (in1 * w22_16 + in2 * w22_17 + in3 * w22_18 + in4 * w11_1 + in5 * w11_2 + in6 * w12_1 + in7 * w12_2 + in8 * w13_1 + in9 * w13_2) >>> 8 + b2_11;\n",
        "# assign out14 = (in1 * w14_1 + in2 * w14_2 + in3 * w15_1 + in4 * w15_2 + in5 * w16_1 + in6 * w16_2 + in7 * w17_1 + in8 * w17_2 + in9 * w18_1) >>> 8 + b2_12;\n",
        "# assign out15 = (in1 * w18_2 + in2 * w19_1 + in3 * w19_2 + in4 * w110_1 + in5 * w110_2 + in6 * w21_1 + in7 * w21_2 + in8 * w21_3 + in9 * w21_4) >>> 8 + b2_13;\n",
        "# assign out16 = (in1 * w21_5 + in2 * w21_6 + in3 * w21_7 + in4 * w21_8 + in5 * w21_9 + in6 * w21_10 + in7 * w21_11 + in8 * w21_12 + in9 * w21_13) >>> 8 + b2_14;\n",
        "# assign out17 = (in1 * w21_14 + in2 * w21_15 + in3 * w21_16 + in4 * w21_17 + in5 * w21_18 + in6 * w22_1 + in7 * w22_2 + in8 * w22_3 + in9 * w22_4) >>> 8 + b2_15;\n",
        "# assign out18 = (in1 * w22_5 + in2 * w22_6 + in3 * w22_7 + in4 * w22_8 + in5 * w22_9 + in6 * w22_10 + in7 * w22_11 + in8 * w22_12 + in9 * w22_13) >>> 8 + b2_16;\n",
        "\n",
        "# endmodule"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}